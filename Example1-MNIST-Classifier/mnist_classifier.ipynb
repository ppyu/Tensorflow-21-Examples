{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-b3faa95c200a>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# 下载并加载数据\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# s\n",
    "mnist = input_data.read_data_sets(\"./MNIST_data\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(55000, 10)\n"
     ]
    }
   ],
   "source": [
    "# 查看训练数据的大小\n",
    "print(mnist.train.images.shape)\n",
    "print(mnist.train.labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 784)\n",
      "(5000, 10)\n"
     ]
    }
   ],
   "source": [
    "# 查看验证数据的大小\n",
    "print(mnist.validation.images.shape)\n",
    "print(mnist.validation.labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# 查看测试数据的大小\n",
    "print(mnist.test.images.shape)\n",
    "print(mnist.test.labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.3803922  0.37647063 0.3019608\n",
      " 0.46274513 0.2392157  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.3529412\n",
      " 0.5411765  0.9215687  0.9215687  0.9215687  0.9215687  0.9215687\n",
      " 0.9215687  0.9843138  0.9843138  0.9725491  0.9960785  0.9607844\n",
      " 0.9215687  0.74509805 0.08235294 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.54901963 0.9843138  0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.7411765  0.09019608 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.8862746  0.9960785  0.81568635 0.7803922  0.7803922  0.7803922\n",
      " 0.7803922  0.54509807 0.2392157  0.2392157  0.2392157  0.2392157\n",
      " 0.2392157  0.5019608  0.8705883  0.9960785  0.9960785  0.7411765\n",
      " 0.08235294 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.14901961 0.32156864\n",
      " 0.0509804  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.13333334 0.8352942  0.9960785  0.9960785  0.45098042 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.32941177\n",
      " 0.9960785  0.9960785  0.9176471  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.32941177 0.9960785  0.9960785\n",
      " 0.9176471  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4156863  0.6156863  0.9960785  0.9960785  0.95294124 0.20000002\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.09803922\n",
      " 0.45882356 0.8941177  0.8941177  0.8941177  0.9921569  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.94117653 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.26666668 0.4666667  0.86274517 0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.5568628  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.14509805 0.73333335 0.9921569\n",
      " 0.9960785  0.9960785  0.9960785  0.8745099  0.8078432  0.8078432\n",
      " 0.29411766 0.26666668 0.8431373  0.9960785  0.9960785  0.45882356\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4431373  0.8588236  0.9960785  0.9490197  0.89019614 0.45098042\n",
      " 0.34901962 0.12156864 0.         0.         0.         0.\n",
      " 0.7843138  0.9960785  0.9450981  0.16078432 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.6627451  0.9960785\n",
      " 0.6901961  0.24313727 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.18823531 0.9058824  0.9960785\n",
      " 0.9176471  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.07058824 0.48627454 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.32941177 0.9960785  0.9960785  0.6509804  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.54509807\n",
      " 0.9960785  0.9333334  0.22352943 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.8235295  0.9803922  0.9960785  0.65882355\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.9490197  0.9960785  0.93725497 0.22352943 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.34901962 0.9843138  0.9450981\n",
      " 0.3372549  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01960784 0.8078432  0.96470594 0.6156863  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01568628 0.45882356\n",
      " 0.27058825 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# 取值范围：0~1，越接近0->白色，越接近1->黑色\n",
    "print(mnist.train.images[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(mnist.train.labels[1])\n",
    "plt.imshow(mnist.train.images[1,:].reshape(28,28),cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.argmax(mnist.train.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,[None,784])\n",
    "\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# 模型输出\n",
    "y = tf.nn.softmax(tf.matmul(x,W)+b,axis=-1)\n",
    "\n",
    "# 实际的标签\n",
    "y_ = tf.placeholder(tf.float32,[None,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于softmax模型，通常使用交叉熵作为损失函数\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y)))\n",
    "# 定义准确率\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow默认会对所有变量计算梯度\n",
    "# 本模型变量即 W、b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个Session。只有在Session中才能运行优化步骤train_step\n",
    "sess = tf.InteractiveSession()\n",
    "# 运行之前初始化所有变量，分配内存\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_step 1,\ttest_acc:0.9150000214576721,\ttest_loss:2934.7080078125 \n",
      "train_step 2,\ttest_acc:0.9118000268936157,\ttest_loss:3046.30615234375 \n",
      "train_step 3,\ttest_acc:0.9126999974250793,\ttest_loss:3097.85791015625 \n",
      "train_step 4,\ttest_acc:0.8925999999046326,\ttest_loss:3571.30419921875 \n",
      "train_step 5,\ttest_acc:0.9160000085830688,\ttest_loss:2865.96142578125 \n",
      "train_step 6,\ttest_acc:0.9165999889373779,\ttest_loss:2894.146484375 \n",
      "train_step 7,\ttest_acc:0.9106000065803528,\ttest_loss:3081.899169921875 \n",
      "train_step 8,\ttest_acc:0.9168999791145325,\ttest_loss:2866.72314453125 \n",
      "train_step 9,\ttest_acc:0.9143000245094299,\ttest_loss:2935.19287109375 \n",
      "train_step 10,\ttest_acc:0.9179999828338623,\ttest_loss:2891.235595703125 \n",
      "train_step 11,\ttest_acc:0.9031999707221985,\ttest_loss:3200.187744140625 \n",
      "train_step 12,\ttest_acc:0.9016000032424927,\ttest_loss:3472.43408203125 \n",
      "train_step 13,\ttest_acc:0.9111999869346619,\ttest_loss:3006.077880859375 \n",
      "train_step 14,\ttest_acc:0.9169999957084656,\ttest_loss:3002.19873046875 \n",
      "train_step 15,\ttest_acc:0.9121000170707703,\ttest_loss:2983.479736328125 \n",
      "train_step 16,\ttest_acc:0.911300003528595,\ttest_loss:3172.978515625 \n",
      "train_step 17,\ttest_acc:0.9086999893188477,\ttest_loss:3133.5634765625 \n",
      "train_step 18,\ttest_acc:0.9204999804496765,\ttest_loss:2852.201416015625 \n",
      "train_step 19,\ttest_acc:0.9180999994277954,\ttest_loss:2901.36181640625 \n",
      "train_step 20,\ttest_acc:0.911899983882904,\ttest_loss:3052.078125 \n",
      "train_step 21,\ttest_acc:0.9194999933242798,\ttest_loss:2930.7021484375 \n",
      "train_step 22,\ttest_acc:0.9132999777793884,\ttest_loss:3000.920166015625 \n",
      "train_step 23,\ttest_acc:0.9171000123023987,\ttest_loss:2931.32421875 \n",
      "train_step 24,\ttest_acc:0.9089999794960022,\ttest_loss:3103.920654296875 \n",
      "train_step 25,\ttest_acc:0.920199990272522,\ttest_loss:2849.250732421875 \n",
      "train_step 26,\ttest_acc:0.9168000221252441,\ttest_loss:2982.504150390625 \n",
      "train_step 27,\ttest_acc:0.9189000129699707,\ttest_loss:2851.491455078125 \n",
      "train_step 28,\ttest_acc:0.9194999933242798,\ttest_loss:2860.324951171875 \n",
      "train_step 29,\ttest_acc:0.914900004863739,\ttest_loss:3084.081298828125 \n",
      "train_step 30,\ttest_acc:0.9193000197410583,\ttest_loss:2914.66845703125 \n",
      "train_step 31,\ttest_acc:0.920199990272522,\ttest_loss:2853.451904296875 \n",
      "train_step 32,\ttest_acc:0.9204999804496765,\ttest_loss:2900.57275390625 \n",
      "train_step 33,\ttest_acc:0.9185000061988831,\ttest_loss:2863.28564453125 \n",
      "train_step 34,\ttest_acc:0.911899983882904,\ttest_loss:3085.3603515625 \n",
      "train_step 35,\ttest_acc:0.9143999814987183,\ttest_loss:3082.998291015625 \n",
      "train_step 36,\ttest_acc:0.9228000044822693,\ttest_loss:2830.490478515625 \n",
      "train_step 37,\ttest_acc:0.9218000173568726,\ttest_loss:2908.544677734375 \n",
      "train_step 38,\ttest_acc:0.9228000044822693,\ttest_loss:2856.27783203125 \n",
      "train_step 39,\ttest_acc:0.9157000184059143,\ttest_loss:3049.92724609375 \n",
      "train_step 40,\ttest_acc:0.9171000123023987,\ttest_loss:2959.21630859375 \n",
      "train_step 41,\ttest_acc:0.9182000160217285,\ttest_loss:2910.6640625 \n",
      "train_step 42,\ttest_acc:0.9157999753952026,\ttest_loss:2957.80712890625 \n",
      "train_step 43,\ttest_acc:0.9180999994277954,\ttest_loss:2927.95751953125 \n",
      "train_step 44,\ttest_acc:0.9045000076293945,\ttest_loss:3277.48486328125 \n",
      "train_step 45,\ttest_acc:0.9192000031471252,\ttest_loss:2926.390625 \n",
      "train_step 46,\ttest_acc:0.91839998960495,\ttest_loss:2905.415283203125 \n",
      "train_step 47,\ttest_acc:0.9182000160217285,\ttest_loss:2926.132080078125 \n",
      "train_step 48,\ttest_acc:0.9136999845504761,\ttest_loss:3084.788818359375 \n",
      "train_step 49,\ttest_acc:0.9136999845504761,\ttest_loss:3030.895751953125 \n",
      "train_step 50,\ttest_acc:0.919700026512146,\ttest_loss:2909.673095703125 \n",
      "train_step 51,\ttest_acc:0.9228000044822693,\ttest_loss:2820.8212890625 \n",
      "train_step 52,\ttest_acc:0.9208999872207642,\ttest_loss:2864.73681640625 \n",
      "train_step 53,\ttest_acc:0.9146000146865845,\ttest_loss:3002.63671875 \n",
      "train_step 54,\ttest_acc:0.9214000105857849,\ttest_loss:2818.09716796875 \n",
      "train_step 55,\ttest_acc:0.9222000241279602,\ttest_loss:2817.15771484375 \n",
      "train_step 56,\ttest_acc:0.9150000214576721,\ttest_loss:2968.701416015625 \n",
      "train_step 57,\ttest_acc:0.9222000241279602,\ttest_loss:2798.076904296875 \n",
      "train_step 58,\ttest_acc:0.9229000210762024,\ttest_loss:2764.2158203125 \n",
      "train_step 59,\ttest_acc:0.9230999946594238,\ttest_loss:2789.754150390625 \n",
      "train_step 60,\ttest_acc:0.920799970626831,\ttest_loss:2903.80078125 \n",
      "train_step 61,\ttest_acc:0.9217000007629395,\ttest_loss:2911.21044921875 \n",
      "train_step 62,\ttest_acc:0.9151999950408936,\ttest_loss:3016.878662109375 \n",
      "train_step 63,\ttest_acc:0.9126999974250793,\ttest_loss:3086.205810546875 \n",
      "train_step 64,\ttest_acc:0.9225999712944031,\ttest_loss:2840.96533203125 \n",
      "train_step 65,\ttest_acc:0.9239000082015991,\ttest_loss:2826.40771484375 \n",
      "train_step 66,\ttest_acc:0.9218000173568726,\ttest_loss:2862.4892578125 \n",
      "train_step 67,\ttest_acc:0.9179999828338623,\ttest_loss:2983.97998046875 \n",
      "train_step 68,\ttest_acc:0.9172999858856201,\ttest_loss:2933.296630859375 \n",
      "train_step 69,\ttest_acc:0.9223999977111816,\ttest_loss:2829.70654296875 \n",
      "train_step 70,\ttest_acc:0.9125000238418579,\ttest_loss:3092.313720703125 \n",
      "train_step 71,\ttest_acc:0.9229000210762024,\ttest_loss:2858.01904296875 \n",
      "train_step 72,\ttest_acc:0.9172000288963318,\ttest_loss:2991.35986328125 \n",
      "train_step 73,\ttest_acc:0.9239000082015991,\ttest_loss:2806.22021484375 \n",
      "train_step 74,\ttest_acc:0.919700026512146,\ttest_loss:2888.630615234375 \n",
      "train_step 75,\ttest_acc:0.91839998960495,\ttest_loss:2979.568359375 \n",
      "train_step 76,\ttest_acc:0.9235000014305115,\ttest_loss:2848.130126953125 \n",
      "train_step 77,\ttest_acc:0.9153000116348267,\ttest_loss:3178.264892578125 \n",
      "train_step 78,\ttest_acc:0.9186999797821045,\ttest_loss:2958.962158203125 \n",
      "train_step 79,\ttest_acc:0.9226999878883362,\ttest_loss:2884.6318359375 \n",
      "train_step 80,\ttest_acc:0.9187999963760376,\ttest_loss:2907.55078125 \n",
      "train_step 81,\ttest_acc:0.9200000166893005,\ttest_loss:2912.77587890625 \n",
      "train_step 82,\ttest_acc:0.9211999773979187,\ttest_loss:2940.34423828125 \n",
      "train_step 83,\ttest_acc:0.9120000004768372,\ttest_loss:3209.948974609375 \n",
      "train_step 84,\ttest_acc:0.9160000085830688,\ttest_loss:2998.63525390625 \n",
      "train_step 85,\ttest_acc:0.9211000204086304,\ttest_loss:2864.45849609375 \n",
      "train_step 86,\ttest_acc:0.9187999963760376,\ttest_loss:2925.69287109375 \n",
      "train_step 87,\ttest_acc:0.9144999980926514,\ttest_loss:3049.544921875 \n",
      "train_step 88,\ttest_acc:0.9175000190734863,\ttest_loss:2960.020263671875 \n",
      "train_step 89,\ttest_acc:0.906499981880188,\ttest_loss:3220.317138671875 \n",
      "train_step 90,\ttest_acc:0.9157999753952026,\ttest_loss:2933.258544921875 \n",
      "train_step 91,\ttest_acc:0.9207000136375427,\ttest_loss:2839.171142578125 \n",
      "train_step 92,\ttest_acc:0.920799970626831,\ttest_loss:2865.02490234375 \n",
      "train_step 93,\ttest_acc:0.9221000075340271,\ttest_loss:2840.26123046875 \n",
      "train_step 94,\ttest_acc:0.9178000092506409,\ttest_loss:2934.94921875 \n",
      "train_step 95,\ttest_acc:0.9154999852180481,\ttest_loss:2985.02294921875 \n",
      "train_step 96,\ttest_acc:0.9178000092506409,\ttest_loss:2968.668701171875 \n",
      "train_step 97,\ttest_acc:0.9218000173568726,\ttest_loss:2908.037841796875 \n",
      "train_step 98,\ttest_acc:0.9168000221252441,\ttest_loss:3068.332275390625 \n",
      "train_step 99,\ttest_acc:0.9186000227928162,\ttest_loss:2944.87939453125 \n",
      "train_step 100,\ttest_acc:0.9111999869346619,\ttest_loss:3267.56640625 \n",
      "train_step 101,\ttest_acc:0.9169999957084656,\ttest_loss:2998.45849609375 \n",
      "train_step 102,\ttest_acc:0.916700005531311,\ttest_loss:3058.088134765625 \n",
      "train_step 103,\ttest_acc:0.9232000112533569,\ttest_loss:2869.231201171875 \n",
      "train_step 104,\ttest_acc:0.9233999848365784,\ttest_loss:2835.12646484375 \n",
      "train_step 105,\ttest_acc:0.9230999946594238,\ttest_loss:2868.567626953125 \n",
      "train_step 106,\ttest_acc:0.9240000247955322,\ttest_loss:2822.09375 \n",
      "train_step 107,\ttest_acc:0.9211000204086304,\ttest_loss:2875.9052734375 \n",
      "train_step 108,\ttest_acc:0.9140999913215637,\ttest_loss:3196.55908203125 \n",
      "train_step 109,\ttest_acc:0.9229000210762024,\ttest_loss:2884.195556640625 \n",
      "train_step 110,\ttest_acc:0.9146999716758728,\ttest_loss:3072.951171875 \n",
      "train_step 111,\ttest_acc:0.9110000133514404,\ttest_loss:3128.90380859375 \n",
      "train_step 112,\ttest_acc:0.9221000075340271,\ttest_loss:2880.12060546875 \n",
      "train_step 113,\ttest_acc:0.9221000075340271,\ttest_loss:2861.797119140625 \n",
      "train_step 114,\ttest_acc:0.9093999862670898,\ttest_loss:3188.22314453125 \n",
      "train_step 115,\ttest_acc:0.9158999919891357,\ttest_loss:2974.927734375 \n",
      "train_step 116,\ttest_acc:0.9211999773979187,\ttest_loss:2847.9560546875 \n",
      "train_step 117,\ttest_acc:0.9221000075340271,\ttest_loss:2807.0703125 \n",
      "train_step 118,\ttest_acc:0.9171000123023987,\ttest_loss:2908.376953125 \n",
      "train_step 119,\ttest_acc:0.9205999970436096,\ttest_loss:2874.69091796875 \n",
      "train_step 120,\ttest_acc:0.9200000166893005,\ttest_loss:2877.42626953125 \n",
      "train_step 121,\ttest_acc:0.9211999773979187,\ttest_loss:2923.819580078125 \n",
      "train_step 122,\ttest_acc:0.9243000149726868,\ttest_loss:2838.015625 \n",
      "train_step 123,\ttest_acc:0.9194999933242798,\ttest_loss:2945.174560546875 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_step 124,\ttest_acc:0.9072999954223633,\ttest_loss:3188.7314453125 \n",
      "train_step 125,\ttest_acc:0.920799970626831,\ttest_loss:2824.012939453125 \n",
      "train_step 126,\ttest_acc:0.9229999780654907,\ttest_loss:2794.845703125 \n",
      "train_step 127,\ttest_acc:0.919700026512146,\ttest_loss:2913.354248046875 \n",
      "train_step 128,\ttest_acc:0.9204000234603882,\ttest_loss:2962.988037109375 \n",
      "train_step 129,\ttest_acc:0.9254999756813049,\ttest_loss:2822.8515625 \n",
      "train_step 130,\ttest_acc:0.9222000241279602,\ttest_loss:2902.17431640625 \n",
      "train_step 131,\ttest_acc:0.9225000143051147,\ttest_loss:2834.786865234375 \n",
      "train_step 132,\ttest_acc:0.9228000044822693,\ttest_loss:2891.892578125 \n",
      "train_step 133,\ttest_acc:0.9182999730110168,\ttest_loss:2936.177734375 \n",
      "train_step 134,\ttest_acc:0.9193000197410583,\ttest_loss:2875.707275390625 \n",
      "train_step 135,\ttest_acc:0.9182000160217285,\ttest_loss:2922.45654296875 \n",
      "train_step 136,\ttest_acc:0.9218000173568726,\ttest_loss:2869.044921875 \n",
      "train_step 137,\ttest_acc:0.9214000105857849,\ttest_loss:2776.599853515625 \n",
      "train_step 138,\ttest_acc:0.9222000241279602,\ttest_loss:2783.166015625 \n",
      "train_step 139,\ttest_acc:0.9180999994277954,\ttest_loss:2884.415771484375 \n",
      "train_step 140,\ttest_acc:0.9139999747276306,\ttest_loss:3018.19189453125 \n",
      "train_step 141,\ttest_acc:0.9111999869346619,\ttest_loss:3008.0 \n",
      "train_step 142,\ttest_acc:0.9143000245094299,\ttest_loss:3084.517578125 \n",
      "train_step 143,\ttest_acc:0.9086999893188477,\ttest_loss:3263.95068359375 \n",
      "train_step 144,\ttest_acc:0.9179999828338623,\ttest_loss:2960.85400390625 \n",
      "train_step 145,\ttest_acc:0.9186999797821045,\ttest_loss:2925.973388671875 \n",
      "train_step 146,\ttest_acc:0.9187999963760376,\ttest_loss:2968.349853515625 \n",
      "train_step 147,\ttest_acc:0.9199000000953674,\ttest_loss:2915.568603515625 \n",
      "train_step 148,\ttest_acc:0.9154999852180481,\ttest_loss:3002.48095703125 \n",
      "train_step 149,\ttest_acc:0.9223999977111816,\ttest_loss:2840.52001953125 \n",
      "train_step 150,\ttest_acc:0.9182000160217285,\ttest_loss:2948.671630859375 \n",
      "train_step 151,\ttest_acc:0.9186999797821045,\ttest_loss:2899.373046875 \n",
      "train_step 152,\ttest_acc:0.9215999841690063,\ttest_loss:2802.718017578125 \n",
      "train_step 153,\ttest_acc:0.9128999710083008,\ttest_loss:3073.43408203125 \n",
      "train_step 154,\ttest_acc:0.9169999957084656,\ttest_loss:2983.62548828125 \n",
      "train_step 155,\ttest_acc:0.9182000160217285,\ttest_loss:2866.44873046875 \n",
      "train_step 156,\ttest_acc:0.9207000136375427,\ttest_loss:2905.259765625 \n",
      "train_step 157,\ttest_acc:0.906000018119812,\ttest_loss:3261.3984375 \n",
      "train_step 158,\ttest_acc:0.9204999804496765,\ttest_loss:2908.20068359375 \n",
      "train_step 159,\ttest_acc:0.9164999723434448,\ttest_loss:2906.536376953125 \n",
      "train_step 160,\ttest_acc:0.9182999730110168,\ttest_loss:2933.8701171875 \n",
      "train_step 161,\ttest_acc:0.9222999811172485,\ttest_loss:2799.001953125 \n",
      "train_step 162,\ttest_acc:0.9200999736785889,\ttest_loss:2844.415283203125 \n",
      "train_step 163,\ttest_acc:0.9230999946594238,\ttest_loss:2824.76611328125 \n",
      "train_step 164,\ttest_acc:0.9212999939918518,\ttest_loss:2852.567138671875 \n",
      "train_step 165,\ttest_acc:0.9186999797821045,\ttest_loss:2884.986328125 \n",
      "train_step 166,\ttest_acc:0.9205999970436096,\ttest_loss:2866.185302734375 \n",
      "train_step 167,\ttest_acc:0.9207000136375427,\ttest_loss:2852.86083984375 \n",
      "train_step 168,\ttest_acc:0.9150999784469604,\ttest_loss:3001.12646484375 \n",
      "train_step 169,\ttest_acc:0.9197999835014343,\ttest_loss:2867.921630859375 \n",
      "train_step 170,\ttest_acc:0.9185000061988831,\ttest_loss:2927.1787109375 \n",
      "train_step 171,\ttest_acc:0.9067000150680542,\ttest_loss:3277.7578125 \n",
      "train_step 172,\ttest_acc:0.9187999963760376,\ttest_loss:2920.3330078125 \n",
      "train_step 173,\ttest_acc:0.9174000024795532,\ttest_loss:2949.87158203125 \n",
      "train_step 174,\ttest_acc:0.923799991607666,\ttest_loss:2832.794921875 \n",
      "train_step 175,\ttest_acc:0.9196000099182129,\ttest_loss:2927.06005859375 \n",
      "train_step 176,\ttest_acc:0.9144999980926514,\ttest_loss:3039.604248046875 \n",
      "train_step 177,\ttest_acc:0.9222000241279602,\ttest_loss:2883.1591796875 \n",
      "train_step 178,\ttest_acc:0.9204000234603882,\ttest_loss:2912.420654296875 \n",
      "train_step 179,\ttest_acc:0.9203000068664551,\ttest_loss:2861.950439453125 \n",
      "train_step 180,\ttest_acc:0.9146000146865845,\ttest_loss:3060.6435546875 \n",
      "train_step 181,\ttest_acc:0.9162999987602234,\ttest_loss:2974.569580078125 \n",
      "train_step 182,\ttest_acc:0.9197999835014343,\ttest_loss:2939.01123046875 \n",
      "train_step 183,\ttest_acc:0.9196000099182129,\ttest_loss:2914.21875 \n",
      "train_step 184,\ttest_acc:0.9114000201225281,\ttest_loss:3215.54345703125 \n",
      "train_step 185,\ttest_acc:0.9225999712944031,\ttest_loss:2853.79638671875 \n",
      "train_step 186,\ttest_acc:0.923799991607666,\ttest_loss:2817.6171875 \n",
      "train_step 187,\ttest_acc:0.9229999780654907,\ttest_loss:2819.792236328125 \n",
      "train_step 188,\ttest_acc:0.9210000038146973,\ttest_loss:2923.138427734375 \n",
      "train_step 189,\ttest_acc:0.9210000038146973,\ttest_loss:2858.037109375 \n",
      "train_step 190,\ttest_acc:0.9060999751091003,\ttest_loss:3260.69287109375 \n",
      "train_step 191,\ttest_acc:0.9136999845504761,\ttest_loss:3118.8486328125 \n",
      "train_step 192,\ttest_acc:0.9221000075340271,\ttest_loss:2893.32763671875 \n",
      "train_step 193,\ttest_acc:0.9222999811172485,\ttest_loss:2881.16552734375 \n",
      "train_step 194,\ttest_acc:0.92330002784729,\ttest_loss:2813.030029296875 \n",
      "train_step 195,\ttest_acc:0.919700026512146,\ttest_loss:2944.988525390625 \n",
      "train_step 196,\ttest_acc:0.90829998254776,\ttest_loss:3176.901611328125 \n",
      "train_step 197,\ttest_acc:0.9154999852180481,\ttest_loss:3117.32080078125 \n",
      "train_step 198,\ttest_acc:0.9004999995231628,\ttest_loss:3540.54541015625 \n",
      "train_step 199,\ttest_acc:0.9151999950408936,\ttest_loss:2981.55615234375 \n",
      "train_step 200,\ttest_acc:0.9235000014305115,\ttest_loss:2785.261962890625 \n",
      "train_step 201,\ttest_acc:0.9229000210762024,\ttest_loss:2822.037841796875 \n",
      "train_step 202,\ttest_acc:0.9232000112533569,\ttest_loss:2840.9365234375 \n",
      "train_step 203,\ttest_acc:0.9207000136375427,\ttest_loss:2915.19775390625 \n",
      "train_step 204,\ttest_acc:0.9230999946594238,\ttest_loss:2798.83154296875 \n",
      "train_step 205,\ttest_acc:0.9253000020980835,\ttest_loss:2792.78369140625 \n",
      "train_step 206,\ttest_acc:0.920199990272522,\ttest_loss:2902.884765625 \n",
      "train_step 207,\ttest_acc:0.9222999811172485,\ttest_loss:2831.72705078125 \n",
      "train_step 208,\ttest_acc:0.9228000044822693,\ttest_loss:2837.955322265625 \n",
      "train_step 209,\ttest_acc:0.9240999817848206,\ttest_loss:2779.50439453125 \n",
      "train_step 210,\ttest_acc:0.9164000153541565,\ttest_loss:2993.0673828125 \n",
      "train_step 211,\ttest_acc:0.923799991607666,\ttest_loss:2765.505126953125 \n",
      "train_step 212,\ttest_acc:0.9211000204086304,\ttest_loss:2889.3544921875 \n",
      "train_step 213,\ttest_acc:0.9232000112533569,\ttest_loss:2767.107421875 \n",
      "train_step 214,\ttest_acc:0.8744000196456909,\ttest_loss:3961.521728515625 \n",
      "train_step 215,\ttest_acc:0.8992999792098999,\ttest_loss:3435.175537109375 \n",
      "train_step 216,\ttest_acc:0.9172000288963318,\ttest_loss:2907.761962890625 \n",
      "train_step 217,\ttest_acc:0.9194999933242798,\ttest_loss:2861.296630859375 \n",
      "train_step 218,\ttest_acc:0.9147999882698059,\ttest_loss:3021.20263671875 \n",
      "train_step 219,\ttest_acc:0.9157999753952026,\ttest_loss:3058.628662109375 \n",
      "train_step 220,\ttest_acc:0.9200000166893005,\ttest_loss:2857.992431640625 \n",
      "train_step 221,\ttest_acc:0.9157999753952026,\ttest_loss:2938.19384765625 \n",
      "train_step 222,\ttest_acc:0.9180999994277954,\ttest_loss:3001.23291015625 \n",
      "train_step 223,\ttest_acc:0.9190999865531921,\ttest_loss:2896.114990234375 \n",
      "train_step 224,\ttest_acc:0.9129999876022339,\ttest_loss:3069.0244140625 \n",
      "train_step 225,\ttest_acc:0.9208999872207642,\ttest_loss:2843.742919921875 \n",
      "train_step 226,\ttest_acc:0.9203000068664551,\ttest_loss:2854.656982421875 \n",
      "train_step 227,\ttest_acc:0.9247000217437744,\ttest_loss:2849.102783203125 \n",
      "train_step 228,\ttest_acc:0.9128000140190125,\ttest_loss:3077.704833984375 \n",
      "train_step 229,\ttest_acc:0.9114000201225281,\ttest_loss:3065.9267578125 \n",
      "train_step 230,\ttest_acc:0.920199990272522,\ttest_loss:2897.55908203125 \n",
      "train_step 231,\ttest_acc:0.9142000079154968,\ttest_loss:3058.42236328125 \n",
      "train_step 232,\ttest_acc:0.9171000123023987,\ttest_loss:3026.60400390625 \n",
      "train_step 233,\ttest_acc:0.9154000282287598,\ttest_loss:3025.506591796875 \n",
      "train_step 234,\ttest_acc:0.9221000075340271,\ttest_loss:2791.65625 \n",
      "train_step 235,\ttest_acc:0.9164000153541565,\ttest_loss:2955.635009765625 \n",
      "train_step 236,\ttest_acc:0.9222999811172485,\ttest_loss:2830.263671875 \n",
      "train_step 237,\ttest_acc:0.9210000038146973,\ttest_loss:2863.793701171875 \n",
      "train_step 238,\ttest_acc:0.9225000143051147,\ttest_loss:2805.307861328125 \n",
      "train_step 239,\ttest_acc:0.9143999814987183,\ttest_loss:2979.43310546875 \n",
      "train_step 240,\ttest_acc:0.9174000024795532,\ttest_loss:2895.47509765625 \n",
      "train_step 241,\ttest_acc:0.9160000085830688,\ttest_loss:2928.265625 \n",
      "train_step 242,\ttest_acc:0.9179999828338623,\ttest_loss:2882.19580078125 \n",
      "train_step 243,\ttest_acc:0.9147999882698059,\ttest_loss:2997.85986328125 \n",
      "train_step 244,\ttest_acc:0.9204000234603882,\ttest_loss:2945.3828125 \n",
      "train_step 245,\ttest_acc:0.9228000044822693,\ttest_loss:2795.0107421875 \n",
      "train_step 246,\ttest_acc:0.916700005531311,\ttest_loss:2947.361572265625 \n",
      "train_step 247,\ttest_acc:0.9168000221252441,\ttest_loss:3011.61962890625 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_step 248,\ttest_acc:0.9139000177383423,\ttest_loss:3094.6435546875 \n",
      "train_step 249,\ttest_acc:0.917900025844574,\ttest_loss:2942.95703125 \n",
      "train_step 250,\ttest_acc:0.9121000170707703,\ttest_loss:3035.979248046875 \n",
      "train_step 251,\ttest_acc:0.917900025844574,\ttest_loss:2887.50341796875 \n",
      "train_step 252,\ttest_acc:0.9229999780654907,\ttest_loss:2796.34033203125 \n",
      "train_step 253,\ttest_acc:0.9265999794006348,\ttest_loss:2767.631103515625 \n",
      "train_step 254,\ttest_acc:0.919700026512146,\ttest_loss:2952.681640625 \n",
      "train_step 255,\ttest_acc:0.9164999723434448,\ttest_loss:3021.3525390625 \n",
      "train_step 256,\ttest_acc:0.9208999872207642,\ttest_loss:2925.933349609375 \n",
      "train_step 257,\ttest_acc:0.8913999795913696,\ttest_loss:3770.7890625 \n",
      "train_step 258,\ttest_acc:0.9088000059127808,\ttest_loss:3280.899658203125 \n",
      "train_step 259,\ttest_acc:0.90420001745224,\ttest_loss:3287.37255859375 \n",
      "train_step 260,\ttest_acc:0.921999990940094,\ttest_loss:2871.4814453125 \n",
      "train_step 261,\ttest_acc:0.9235000014305115,\ttest_loss:2877.7626953125 \n",
      "train_step 262,\ttest_acc:0.9211999773979187,\ttest_loss:2962.052978515625 \n",
      "train_step 263,\ttest_acc:0.9232000112533569,\ttest_loss:2854.5087890625 \n",
      "train_step 264,\ttest_acc:0.9228000044822693,\ttest_loss:2860.599609375 \n",
      "train_step 265,\ttest_acc:0.9199000000953674,\ttest_loss:2942.46142578125 \n",
      "train_step 266,\ttest_acc:0.9117000102996826,\ttest_loss:3082.34326171875 \n",
      "train_step 267,\ttest_acc:0.9142000079154968,\ttest_loss:3085.04150390625 \n",
      "train_step 268,\ttest_acc:0.9046000242233276,\ttest_loss:3219.779296875 \n",
      "train_step 269,\ttest_acc:0.9205999970436096,\ttest_loss:2879.82958984375 \n",
      "train_step 270,\ttest_acc:0.9226999878883362,\ttest_loss:2771.1474609375 \n",
      "train_step 271,\ttest_acc:0.9175000190734863,\ttest_loss:2992.46240234375 \n",
      "train_step 272,\ttest_acc:0.9175999760627747,\ttest_loss:2876.54150390625 \n",
      "train_step 273,\ttest_acc:0.9223999977111816,\ttest_loss:2808.1083984375 \n",
      "train_step 274,\ttest_acc:0.9203000068664551,\ttest_loss:2820.406982421875 \n",
      "train_step 275,\ttest_acc:0.9225999712944031,\ttest_loss:2833.83056640625 \n",
      "train_step 276,\ttest_acc:0.9099000096321106,\ttest_loss:3088.47412109375 \n",
      "train_step 277,\ttest_acc:0.8995000123977661,\ttest_loss:3326.637451171875 \n",
      "train_step 278,\ttest_acc:0.916100025177002,\ttest_loss:2902.33203125 \n",
      "train_step 279,\ttest_acc:0.9128999710083008,\ttest_loss:2961.599609375 \n",
      "train_step 280,\ttest_acc:0.9144999980926514,\ttest_loss:2985.6015625 \n",
      "train_step 281,\ttest_acc:0.9185000061988831,\ttest_loss:2884.594482421875 \n",
      "train_step 282,\ttest_acc:0.9189000129699707,\ttest_loss:2891.31103515625 \n",
      "train_step 283,\ttest_acc:0.9150999784469604,\ttest_loss:2930.948974609375 \n",
      "train_step 284,\ttest_acc:0.9161999821662903,\ttest_loss:2901.22607421875 \n",
      "train_step 285,\ttest_acc:0.9211000204086304,\ttest_loss:2795.693359375 \n",
      "train_step 286,\ttest_acc:0.9174000024795532,\ttest_loss:2946.81494140625 \n",
      "train_step 287,\ttest_acc:0.9199000000953674,\ttest_loss:2860.23583984375 \n",
      "train_step 288,\ttest_acc:0.9225000143051147,\ttest_loss:2754.8994140625 \n",
      "train_step 289,\ttest_acc:0.9186999797821045,\ttest_loss:2892.4111328125 \n",
      "train_step 290,\ttest_acc:0.9128000140190125,\ttest_loss:2978.4814453125 \n",
      "train_step 291,\ttest_acc:0.9153000116348267,\ttest_loss:2988.472412109375 \n",
      "train_step 292,\ttest_acc:0.8942000269889832,\ttest_loss:3492.821044921875 \n",
      "train_step 293,\ttest_acc:0.919700026512146,\ttest_loss:2862.262939453125 \n",
      "train_step 294,\ttest_acc:0.9178000092506409,\ttest_loss:2909.758056640625 \n",
      "train_step 295,\ttest_acc:0.9125999808311462,\ttest_loss:3035.82958984375 \n",
      "train_step 296,\ttest_acc:0.9180999994277954,\ttest_loss:2862.47607421875 \n",
      "train_step 297,\ttest_acc:0.9182999730110168,\ttest_loss:2828.53857421875 \n",
      "train_step 298,\ttest_acc:0.9151999950408936,\ttest_loss:3018.4462890625 \n",
      "train_step 299,\ttest_acc:0.9205999970436096,\ttest_loss:2812.425537109375 \n",
      "train_step 300,\ttest_acc:0.9200000166893005,\ttest_loss:2930.93994140625 \n",
      "train_step 301,\ttest_acc:0.916700005531311,\ttest_loss:2890.49853515625 \n",
      "train_step 302,\ttest_acc:0.919700026512146,\ttest_loss:2880.82861328125 \n",
      "train_step 303,\ttest_acc:0.9207000136375427,\ttest_loss:2877.0361328125 \n",
      "train_step 304,\ttest_acc:0.919700026512146,\ttest_loss:2882.51416015625 \n",
      "train_step 305,\ttest_acc:0.9189000129699707,\ttest_loss:2867.9375 \n",
      "train_step 306,\ttest_acc:0.9171000123023987,\ttest_loss:2926.961181640625 \n",
      "train_step 307,\ttest_acc:0.9218999743461609,\ttest_loss:2824.34765625 \n",
      "train_step 308,\ttest_acc:0.9225000143051147,\ttest_loss:2785.1904296875 \n",
      "train_step 309,\ttest_acc:0.9020000100135803,\ttest_loss:3338.55615234375 \n",
      "train_step 310,\ttest_acc:0.9207000136375427,\ttest_loss:2827.93212890625 \n",
      "train_step 311,\ttest_acc:0.9164000153541565,\ttest_loss:2926.97607421875 \n",
      "train_step 312,\ttest_acc:0.9101999998092651,\ttest_loss:3133.1298828125 \n",
      "train_step 313,\ttest_acc:0.9100000262260437,\ttest_loss:3050.7294921875 \n",
      "train_step 314,\ttest_acc:0.917900025844574,\ttest_loss:2914.114013671875 \n",
      "train_step 315,\ttest_acc:0.9168999791145325,\ttest_loss:2945.73388671875 \n",
      "train_step 316,\ttest_acc:0.9197999835014343,\ttest_loss:2861.658203125 \n",
      "train_step 317,\ttest_acc:0.9132000207901001,\ttest_loss:3107.949951171875 \n",
      "train_step 318,\ttest_acc:0.921999990940094,\ttest_loss:2804.219482421875 \n",
      "train_step 319,\ttest_acc:0.9211000204086304,\ttest_loss:2824.448486328125 \n",
      "train_step 320,\ttest_acc:0.9187999963760376,\ttest_loss:2945.02392578125 \n",
      "train_step 321,\ttest_acc:0.9065999984741211,\ttest_loss:3253.19677734375 \n",
      "train_step 322,\ttest_acc:0.8848000168800354,\ttest_loss:3786.68408203125 \n",
      "train_step 323,\ttest_acc:0.9160000085830688,\ttest_loss:2967.458740234375 \n",
      "train_step 324,\ttest_acc:0.9169999957084656,\ttest_loss:2923.134765625 \n",
      "train_step 325,\ttest_acc:0.9074000120162964,\ttest_loss:3225.8056640625 \n",
      "train_step 326,\ttest_acc:0.9164999723434448,\ttest_loss:2950.762939453125 \n",
      "train_step 327,\ttest_acc:0.9057999849319458,\ttest_loss:3194.55859375 \n",
      "train_step 328,\ttest_acc:0.9088000059127808,\ttest_loss:3095.885009765625 \n",
      "train_step 329,\ttest_acc:0.9144999980926514,\ttest_loss:2941.08544921875 \n",
      "train_step 330,\ttest_acc:0.9182999730110168,\ttest_loss:2890.1318359375 \n",
      "train_step 331,\ttest_acc:0.9179999828338623,\ttest_loss:2916.589599609375 \n",
      "train_step 332,\ttest_acc:0.9171000123023987,\ttest_loss:2911.515869140625 \n",
      "train_step 333,\ttest_acc:0.9121000170707703,\ttest_loss:3061.1318359375 \n",
      "train_step 334,\ttest_acc:0.9089000225067139,\ttest_loss:3098.972900390625 \n",
      "train_step 335,\ttest_acc:0.9172000288963318,\ttest_loss:2905.02685546875 \n",
      "train_step 336,\ttest_acc:0.9200000166893005,\ttest_loss:2874.6416015625 \n",
      "train_step 337,\ttest_acc:0.9225999712944031,\ttest_loss:2780.1611328125 \n",
      "train_step 338,\ttest_acc:0.9031000137329102,\ttest_loss:3313.921142578125 \n",
      "train_step 339,\ttest_acc:0.917900025844574,\ttest_loss:2902.729736328125 \n",
      "train_step 340,\ttest_acc:0.9179999828338623,\ttest_loss:2910.4375 \n",
      "train_step 341,\ttest_acc:0.921999990940094,\ttest_loss:2859.447998046875 \n",
      "train_step 342,\ttest_acc:0.9185000061988831,\ttest_loss:2898.281494140625 \n",
      "train_step 343,\ttest_acc:0.9174000024795532,\ttest_loss:2918.257080078125 \n",
      "train_step 344,\ttest_acc:0.9175999760627747,\ttest_loss:2931.43115234375 \n",
      "train_step 345,\ttest_acc:0.916100025177002,\ttest_loss:3018.024658203125 \n",
      "train_step 346,\ttest_acc:0.9199000000953674,\ttest_loss:2852.36572265625 \n",
      "train_step 347,\ttest_acc:0.9162999987602234,\ttest_loss:2963.424072265625 \n",
      "train_step 348,\ttest_acc:0.9164999723434448,\ttest_loss:2953.00341796875 \n",
      "train_step 349,\ttest_acc:0.916700005531311,\ttest_loss:3004.51123046875 \n",
      "train_step 350,\ttest_acc:0.921500027179718,\ttest_loss:2839.689453125 \n",
      "train_step 351,\ttest_acc:0.9050999879837036,\ttest_loss:3220.665771484375 \n",
      "train_step 352,\ttest_acc:0.916700005531311,\ttest_loss:2948.735107421875 \n",
      "train_step 353,\ttest_acc:0.9043999910354614,\ttest_loss:3262.43359375 \n",
      "train_step 354,\ttest_acc:0.9139000177383423,\ttest_loss:3024.595947265625 \n",
      "train_step 355,\ttest_acc:0.9222999811172485,\ttest_loss:2809.31396484375 \n",
      "train_step 356,\ttest_acc:0.9168999791145325,\ttest_loss:2867.243408203125 \n",
      "train_step 357,\ttest_acc:0.9228000044822693,\ttest_loss:2763.40771484375 \n",
      "train_step 358,\ttest_acc:0.9212999939918518,\ttest_loss:2844.775634765625 \n",
      "train_step 359,\ttest_acc:0.9169999957084656,\ttest_loss:2921.1064453125 \n",
      "train_step 360,\ttest_acc:0.9168999791145325,\ttest_loss:2874.229736328125 \n",
      "train_step 361,\ttest_acc:0.909600019454956,\ttest_loss:3096.98486328125 \n",
      "train_step 362,\ttest_acc:0.9143999814987183,\ttest_loss:2958.2353515625 \n",
      "train_step 363,\ttest_acc:0.9075999855995178,\ttest_loss:3089.945068359375 \n",
      "train_step 364,\ttest_acc:0.9150000214576721,\ttest_loss:2981.48974609375 \n",
      "train_step 365,\ttest_acc:0.9053999781608582,\ttest_loss:3226.974365234375 \n",
      "train_step 366,\ttest_acc:0.9246000051498413,\ttest_loss:2777.965087890625 \n",
      "train_step 367,\ttest_acc:0.9222000241279602,\ttest_loss:2872.64990234375 \n",
      "train_step 368,\ttest_acc:0.9210000038146973,\ttest_loss:2825.426025390625 \n",
      "train_step 369,\ttest_acc:0.9210000038146973,\ttest_loss:2863.1328125 \n",
      "train_step 370,\ttest_acc:0.920799970626831,\ttest_loss:2861.42822265625 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_step 371,\ttest_acc:0.9236999750137329,\ttest_loss:2790.425048828125 \n",
      "train_step 372,\ttest_acc:0.913100004196167,\ttest_loss:2931.0703125 \n",
      "train_step 373,\ttest_acc:0.9197999835014343,\ttest_loss:2816.298828125 \n",
      "train_step 374,\ttest_acc:0.9246000051498413,\ttest_loss:2773.060546875 \n",
      "train_step 375,\ttest_acc:0.9194999933242798,\ttest_loss:2875.93212890625 \n",
      "train_step 376,\ttest_acc:0.9179999828338623,\ttest_loss:2861.64599609375 \n",
      "train_step 377,\ttest_acc:0.9158999919891357,\ttest_loss:2938.960693359375 \n",
      "train_step 378,\ttest_acc:0.9161999821662903,\ttest_loss:2972.847900390625 \n",
      "train_step 379,\ttest_acc:0.921999990940094,\ttest_loss:2878.59375 \n",
      "train_step 380,\ttest_acc:0.923799991607666,\ttest_loss:2770.451416015625 \n",
      "train_step 381,\ttest_acc:0.91839998960495,\ttest_loss:2962.05712890625 \n",
      "train_step 382,\ttest_acc:0.9164999723434448,\ttest_loss:2939.325927734375 \n",
      "train_step 383,\ttest_acc:0.921999990940094,\ttest_loss:2830.505859375 \n",
      "train_step 384,\ttest_acc:0.9186999797821045,\ttest_loss:2892.62646484375 \n",
      "train_step 385,\ttest_acc:0.9221000075340271,\ttest_loss:2811.334228515625 \n",
      "train_step 386,\ttest_acc:0.9150000214576721,\ttest_loss:3014.669677734375 \n",
      "train_step 387,\ttest_acc:0.9121999740600586,\ttest_loss:3036.769287109375 \n",
      "train_step 388,\ttest_acc:0.9157000184059143,\ttest_loss:2960.28857421875 \n",
      "train_step 389,\ttest_acc:0.9189000129699707,\ttest_loss:2913.84228515625 \n",
      "train_step 390,\ttest_acc:0.9218000173568726,\ttest_loss:2830.03857421875 \n",
      "train_step 391,\ttest_acc:0.9197999835014343,\ttest_loss:2851.035400390625 \n",
      "train_step 392,\ttest_acc:0.9157999753952026,\ttest_loss:2936.10400390625 \n",
      "train_step 393,\ttest_acc:0.917900025844574,\ttest_loss:2904.064453125 \n",
      "train_step 394,\ttest_acc:0.9122999906539917,\ttest_loss:3221.449951171875 \n",
      "train_step 395,\ttest_acc:0.9186000227928162,\ttest_loss:2902.296875 \n",
      "train_step 396,\ttest_acc:0.919700026512146,\ttest_loss:2900.13134765625 \n",
      "train_step 397,\ttest_acc:0.9169999957084656,\ttest_loss:2963.3310546875 \n",
      "train_step 398,\ttest_acc:0.9182999730110168,\ttest_loss:2877.111083984375 \n",
      "train_step 399,\ttest_acc:0.9174000024795532,\ttest_loss:2955.0654296875 \n",
      "train_step 400,\ttest_acc:0.9157999753952026,\ttest_loss:2992.806640625 \n",
      "train_step 401,\ttest_acc:0.9190000295639038,\ttest_loss:2887.89501953125 \n",
      "train_step 402,\ttest_acc:0.9210000038146973,\ttest_loss:2874.502197265625 \n",
      "train_step 403,\ttest_acc:0.920799970626831,\ttest_loss:2909.73681640625 \n",
      "train_step 404,\ttest_acc:0.9229999780654907,\ttest_loss:2802.13916015625 \n",
      "train_step 405,\ttest_acc:0.9254999756813049,\ttest_loss:2773.202880859375 \n",
      "train_step 406,\ttest_acc:0.9225000143051147,\ttest_loss:2798.62255859375 \n",
      "train_step 407,\ttest_acc:0.9204999804496765,\ttest_loss:2837.572021484375 \n",
      "train_step 408,\ttest_acc:0.9229999780654907,\ttest_loss:2769.5517578125 \n",
      "train_step 409,\ttest_acc:0.9190999865531921,\ttest_loss:2897.156982421875 \n",
      "train_step 410,\ttest_acc:0.9212999939918518,\ttest_loss:2831.2724609375 \n",
      "train_step 411,\ttest_acc:0.9192000031471252,\ttest_loss:2858.92529296875 \n",
      "train_step 412,\ttest_acc:0.9210000038146973,\ttest_loss:2854.407958984375 \n",
      "train_step 413,\ttest_acc:0.9136999845504761,\ttest_loss:3043.634765625 \n",
      "train_step 414,\ttest_acc:0.919700026512146,\ttest_loss:2861.6875 \n",
      "train_step 415,\ttest_acc:0.920799970626831,\ttest_loss:2794.36083984375 \n",
      "train_step 416,\ttest_acc:0.9164000153541565,\ttest_loss:2991.43408203125 \n",
      "train_step 417,\ttest_acc:0.9208999872207642,\ttest_loss:2830.465576171875 \n",
      "train_step 418,\ttest_acc:0.9212999939918518,\ttest_loss:2786.092529296875 \n",
      "train_step 419,\ttest_acc:0.9192000031471252,\ttest_loss:2914.50048828125 \n",
      "train_step 420,\ttest_acc:0.9139999747276306,\ttest_loss:2978.451416015625 \n",
      "train_step 421,\ttest_acc:0.9158999919891357,\ttest_loss:3094.0947265625 \n",
      "train_step 422,\ttest_acc:0.9171000123023987,\ttest_loss:2928.209228515625 \n",
      "train_step 423,\ttest_acc:0.9171000123023987,\ttest_loss:2903.3291015625 \n",
      "train_step 424,\ttest_acc:0.91839998960495,\ttest_loss:2951.879150390625 \n",
      "train_step 425,\ttest_acc:0.9221000075340271,\ttest_loss:2780.218505859375 \n",
      "train_step 426,\ttest_acc:0.9214000105857849,\ttest_loss:2826.993896484375 \n",
      "train_step 427,\ttest_acc:0.9186000227928162,\ttest_loss:2867.458740234375 \n",
      "train_step 428,\ttest_acc:0.9222999811172485,\ttest_loss:2818.2177734375 \n",
      "train_step 429,\ttest_acc:0.9079999923706055,\ttest_loss:3185.015625 \n",
      "train_step 430,\ttest_acc:0.9157999753952026,\ttest_loss:2992.705810546875 \n",
      "train_step 431,\ttest_acc:0.9186999797821045,\ttest_loss:2929.5810546875 \n",
      "train_step 432,\ttest_acc:0.9161999821662903,\ttest_loss:2940.59619140625 \n",
      "train_step 433,\ttest_acc:0.9200999736785889,\ttest_loss:2791.625 \n",
      "train_step 434,\ttest_acc:0.9221000075340271,\ttest_loss:2799.775634765625 \n",
      "train_step 435,\ttest_acc:0.9009000062942505,\ttest_loss:3311.6865234375 \n",
      "train_step 436,\ttest_acc:0.9218000173568726,\ttest_loss:2834.567626953125 \n",
      "train_step 437,\ttest_acc:0.92330002784729,\ttest_loss:2774.0966796875 \n",
      "train_step 438,\ttest_acc:0.9225000143051147,\ttest_loss:2784.219970703125 \n",
      "train_step 439,\ttest_acc:0.9200999736785889,\ttest_loss:2865.060546875 \n",
      "train_step 440,\ttest_acc:0.9228000044822693,\ttest_loss:2765.593505859375 \n",
      "train_step 441,\ttest_acc:0.917900025844574,\ttest_loss:2945.09521484375 \n",
      "train_step 442,\ttest_acc:0.9205999970436096,\ttest_loss:2835.248046875 \n",
      "train_step 443,\ttest_acc:0.9146999716758728,\ttest_loss:2938.043212890625 \n",
      "train_step 444,\ttest_acc:0.9222000241279602,\ttest_loss:2834.4658203125 \n",
      "train_step 445,\ttest_acc:0.9185000061988831,\ttest_loss:2940.26953125 \n",
      "train_step 446,\ttest_acc:0.9196000099182129,\ttest_loss:2962.008056640625 \n",
      "train_step 447,\ttest_acc:0.9232000112533569,\ttest_loss:2841.1005859375 \n",
      "train_step 448,\ttest_acc:0.9197999835014343,\ttest_loss:2858.4111328125 \n",
      "train_step 449,\ttest_acc:0.9187999963760376,\ttest_loss:2963.20166015625 \n",
      "train_step 450,\ttest_acc:0.9218999743461609,\ttest_loss:2861.98876953125 \n",
      "train_step 451,\ttest_acc:0.9180999994277954,\ttest_loss:2983.152099609375 \n",
      "train_step 452,\ttest_acc:0.90420001745224,\ttest_loss:3299.29248046875 \n",
      "train_step 453,\ttest_acc:0.9190999865531921,\ttest_loss:2896.7255859375 \n",
      "train_step 454,\ttest_acc:0.920799970626831,\ttest_loss:2838.003662109375 \n",
      "train_step 455,\ttest_acc:0.9204999804496765,\ttest_loss:2819.64111328125 \n",
      "train_step 456,\ttest_acc:0.9218000173568726,\ttest_loss:2871.388427734375 \n",
      "train_step 457,\ttest_acc:0.9139000177383423,\ttest_loss:3043.246826171875 \n",
      "train_step 458,\ttest_acc:0.9239000082015991,\ttest_loss:2807.978515625 \n",
      "train_step 459,\ttest_acc:0.9193000197410583,\ttest_loss:2950.407470703125 \n",
      "train_step 460,\ttest_acc:0.9211999773979187,\ttest_loss:2904.5478515625 \n",
      "train_step 461,\ttest_acc:0.9193999767303467,\ttest_loss:2852.87841796875 \n",
      "train_step 462,\ttest_acc:0.921500027179718,\ttest_loss:2840.88037109375 \n",
      "train_step 463,\ttest_acc:0.9185000061988831,\ttest_loss:2947.174560546875 \n",
      "train_step 464,\ttest_acc:0.9178000092506409,\ttest_loss:2900.51513671875 \n",
      "train_step 465,\ttest_acc:0.9175999760627747,\ttest_loss:2932.7783203125 \n",
      "train_step 466,\ttest_acc:0.9172000288963318,\ttest_loss:3045.82177734375 \n",
      "train_step 467,\ttest_acc:0.9157999753952026,\ttest_loss:3046.75390625 \n",
      "train_step 468,\ttest_acc:0.9182999730110168,\ttest_loss:2906.600830078125 \n",
      "train_step 469,\ttest_acc:0.9168999791145325,\ttest_loss:2993.20947265625 \n",
      "train_step 470,\ttest_acc:0.916700005531311,\ttest_loss:3031.466552734375 \n",
      "train_step 471,\ttest_acc:0.9199000000953674,\ttest_loss:2852.5390625 \n",
      "train_step 472,\ttest_acc:0.9210000038146973,\ttest_loss:2886.93212890625 \n",
      "train_step 473,\ttest_acc:0.9203000068664551,\ttest_loss:2928.4033203125 \n",
      "train_step 474,\ttest_acc:0.9085000157356262,\ttest_loss:3140.62158203125 \n",
      "train_step 475,\ttest_acc:0.9042999744415283,\ttest_loss:3364.782470703125 \n",
      "train_step 476,\ttest_acc:0.8935999870300293,\ttest_loss:3634.744140625 \n",
      "train_step 477,\ttest_acc:0.9067000150680542,\ttest_loss:3407.546875 \n",
      "train_step 478,\ttest_acc:0.9178000092506409,\ttest_loss:3013.27880859375 \n",
      "train_step 479,\ttest_acc:0.9146000146865845,\ttest_loss:3097.51220703125 \n",
      "train_step 480,\ttest_acc:0.9233999848365784,\ttest_loss:2838.158935546875 \n",
      "train_step 481,\ttest_acc:0.9168999791145325,\ttest_loss:3007.601318359375 \n",
      "train_step 482,\ttest_acc:0.9203000068664551,\ttest_loss:2902.248046875 \n",
      "train_step 483,\ttest_acc:0.9223999977111816,\ttest_loss:2888.809326171875 \n",
      "train_step 484,\ttest_acc:0.9211999773979187,\ttest_loss:2814.416015625 \n",
      "train_step 485,\ttest_acc:0.9190000295639038,\ttest_loss:2917.202392578125 \n",
      "train_step 486,\ttest_acc:0.9218999743461609,\ttest_loss:2824.11767578125 \n",
      "train_step 487,\ttest_acc:0.9217000007629395,\ttest_loss:2797.155029296875 \n",
      "train_step 488,\ttest_acc:0.9164999723434448,\ttest_loss:2957.86474609375 \n",
      "train_step 489,\ttest_acc:0.9229000210762024,\ttest_loss:2800.3818359375 \n",
      "train_step 490,\ttest_acc:0.9165999889373779,\ttest_loss:2946.39013671875 \n",
      "train_step 491,\ttest_acc:0.9168000221252441,\ttest_loss:2989.5791015625 \n",
      "train_step 492,\ttest_acc:0.9156000018119812,\ttest_loss:2922.16162109375 \n",
      "train_step 493,\ttest_acc:0.9089000225067139,\ttest_loss:3129.393310546875 \n",
      "train_step 494,\ttest_acc:0.8889999985694885,\ttest_loss:3718.02734375 \n",
      "train_step 495,\ttest_acc:0.9179999828338623,\ttest_loss:2933.394287109375 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_step 496,\ttest_acc:0.9240999817848206,\ttest_loss:2804.529296875 \n",
      "train_step 497,\ttest_acc:0.917900025844574,\ttest_loss:2962.8408203125 \n",
      "train_step 498,\ttest_acc:0.921999990940094,\ttest_loss:2864.720703125 \n",
      "train_step 499,\ttest_acc:0.8930000066757202,\ttest_loss:3485.31298828125 \n",
      "train_step 500,\ttest_acc:0.9168999791145325,\ttest_loss:2983.12890625 \n",
      "train_step 501,\ttest_acc:0.916700005531311,\ttest_loss:2913.970458984375 \n",
      "train_step 502,\ttest_acc:0.9226999878883362,\ttest_loss:2771.78271484375 \n",
      "train_step 503,\ttest_acc:0.9192000031471252,\ttest_loss:2906.46533203125 \n",
      "train_step 504,\ttest_acc:0.9221000075340271,\ttest_loss:2780.4375 \n",
      "train_step 505,\ttest_acc:0.9211000204086304,\ttest_loss:2837.078125 \n",
      "train_step 506,\ttest_acc:0.9194999933242798,\ttest_loss:2816.201416015625 \n",
      "train_step 507,\ttest_acc:0.9226999878883362,\ttest_loss:2807.989990234375 \n",
      "train_step 508,\ttest_acc:0.9241999983787537,\ttest_loss:2764.748291015625 \n",
      "train_step 509,\ttest_acc:0.9228000044822693,\ttest_loss:2850.31787109375 \n",
      "train_step 510,\ttest_acc:0.9225999712944031,\ttest_loss:2796.153564453125 \n",
      "train_step 511,\ttest_acc:0.9190000295639038,\ttest_loss:2900.8125 \n",
      "train_step 512,\ttest_acc:0.9121000170707703,\ttest_loss:3085.0048828125 \n",
      "train_step 513,\ttest_acc:0.9171000123023987,\ttest_loss:3022.914306640625 \n",
      "train_step 514,\ttest_acc:0.9199000000953674,\ttest_loss:2938.286376953125 \n",
      "train_step 515,\ttest_acc:0.920199990272522,\ttest_loss:2852.239013671875 \n",
      "train_step 516,\ttest_acc:0.921999990940094,\ttest_loss:2869.363525390625 \n",
      "train_step 517,\ttest_acc:0.913100004196167,\ttest_loss:3002.6103515625 \n",
      "train_step 518,\ttest_acc:0.9196000099182129,\ttest_loss:2899.382568359375 \n",
      "train_step 519,\ttest_acc:0.9176999926567078,\ttest_loss:2914.738037109375 \n",
      "train_step 520,\ttest_acc:0.9218000173568726,\ttest_loss:2846.7666015625 \n",
      "train_step 521,\ttest_acc:0.9205999970436096,\ttest_loss:2819.71728515625 \n",
      "train_step 522,\ttest_acc:0.9187999963760376,\ttest_loss:2929.545166015625 \n",
      "train_step 523,\ttest_acc:0.9176999926567078,\ttest_loss:2978.869384765625 \n",
      "train_step 524,\ttest_acc:0.9211000204086304,\ttest_loss:2905.765625 \n",
      "train_step 525,\ttest_acc:0.9203000068664551,\ttest_loss:2838.169921875 \n",
      "train_step 526,\ttest_acc:0.925000011920929,\ttest_loss:2760.345458984375 \n",
      "train_step 527,\ttest_acc:0.9199000000953674,\ttest_loss:2887.07470703125 \n",
      "train_step 528,\ttest_acc:0.9061999917030334,\ttest_loss:3281.22998046875 \n",
      "train_step 529,\ttest_acc:0.9199000000953674,\ttest_loss:2917.512939453125 \n",
      "train_step 530,\ttest_acc:0.9180999994277954,\ttest_loss:2924.375 \n",
      "train_step 531,\ttest_acc:0.9185000061988831,\ttest_loss:2969.3056640625 \n",
      "train_step 532,\ttest_acc:0.9120000004768372,\ttest_loss:3089.9951171875 \n",
      "train_step 533,\ttest_acc:0.9200999736785889,\ttest_loss:2914.05322265625 \n",
      "train_step 534,\ttest_acc:0.9179999828338623,\ttest_loss:2893.40283203125 \n",
      "train_step 535,\ttest_acc:0.9204999804496765,\ttest_loss:2866.564697265625 \n",
      "train_step 536,\ttest_acc:0.9154000282287598,\ttest_loss:3005.840576171875 \n",
      "train_step 537,\ttest_acc:0.9175999760627747,\ttest_loss:2991.539306640625 \n",
      "train_step 538,\ttest_acc:0.9211999773979187,\ttest_loss:2867.896484375 \n",
      "train_step 539,\ttest_acc:0.9175000190734863,\ttest_loss:2977.30419921875 \n",
      "train_step 540,\ttest_acc:0.9057999849319458,\ttest_loss:3258.5068359375 \n",
      "train_step 541,\ttest_acc:0.9157000184059143,\ttest_loss:2989.704833984375 \n",
      "train_step 542,\ttest_acc:0.9192000031471252,\ttest_loss:2885.328125 \n",
      "train_step 543,\ttest_acc:0.9196000099182129,\ttest_loss:2964.53125 \n",
      "train_step 544,\ttest_acc:0.9150999784469604,\ttest_loss:2969.393798828125 \n",
      "train_step 545,\ttest_acc:0.9135000109672546,\ttest_loss:2970.90380859375 \n",
      "train_step 546,\ttest_acc:0.9085000157356262,\ttest_loss:3297.43212890625 \n",
      "train_step 547,\ttest_acc:0.9121000170707703,\ttest_loss:3172.953125 \n",
      "train_step 548,\ttest_acc:0.9190999865531921,\ttest_loss:2905.970703125 \n",
      "train_step 549,\ttest_acc:0.9182000160217285,\ttest_loss:3000.734375 \n",
      "train_step 550,\ttest_acc:0.9175999760627747,\ttest_loss:2890.099609375 \n",
      "train_step 551,\ttest_acc:0.9203000068664551,\ttest_loss:2942.05419921875 \n",
      "train_step 552,\ttest_acc:0.909500002861023,\ttest_loss:3146.78173828125 \n",
      "train_step 553,\ttest_acc:0.9164999723434448,\ttest_loss:2949.31640625 \n",
      "train_step 554,\ttest_acc:0.9136999845504761,\ttest_loss:3099.843017578125 \n",
      "train_step 555,\ttest_acc:0.917900025844574,\ttest_loss:2988.94482421875 \n",
      "train_step 556,\ttest_acc:0.9186000227928162,\ttest_loss:2873.47509765625 \n",
      "train_step 557,\ttest_acc:0.911899983882904,\ttest_loss:3181.17822265625 \n",
      "train_step 558,\ttest_acc:0.9208999872207642,\ttest_loss:2911.55712890625 \n",
      "train_step 559,\ttest_acc:0.9162999987602234,\ttest_loss:3000.418701171875 \n",
      "train_step 560,\ttest_acc:0.9228000044822693,\ttest_loss:2848.532958984375 \n",
      "train_step 561,\ttest_acc:0.9081000089645386,\ttest_loss:3237.0712890625 \n",
      "train_step 562,\ttest_acc:0.9203000068664551,\ttest_loss:2900.117431640625 \n",
      "train_step 563,\ttest_acc:0.9129999876022339,\ttest_loss:3127.51171875 \n",
      "train_step 564,\ttest_acc:0.9169999957084656,\ttest_loss:2934.7919921875 \n",
      "train_step 565,\ttest_acc:0.9147999882698059,\ttest_loss:2961.81982421875 \n",
      "train_step 566,\ttest_acc:0.913100004196167,\ttest_loss:3055.38037109375 \n",
      "train_step 567,\ttest_acc:0.9114999771118164,\ttest_loss:3073.3974609375 \n",
      "train_step 568,\ttest_acc:0.9189000129699707,\ttest_loss:2873.985595703125 \n",
      "train_step 569,\ttest_acc:0.9182000160217285,\ttest_loss:3004.80908203125 \n",
      "train_step 570,\ttest_acc:0.9210000038146973,\ttest_loss:2881.669921875 \n",
      "train_step 571,\ttest_acc:0.9103000164031982,\ttest_loss:3183.90869140625 \n",
      "train_step 572,\ttest_acc:0.9160000085830688,\ttest_loss:3002.231689453125 \n",
      "train_step 573,\ttest_acc:0.9182999730110168,\ttest_loss:2903.79296875 \n",
      "train_step 574,\ttest_acc:0.919700026512146,\ttest_loss:2833.6669921875 \n",
      "train_step 575,\ttest_acc:0.9215999841690063,\ttest_loss:2792.283203125 \n",
      "train_step 576,\ttest_acc:0.9172999858856201,\ttest_loss:2880.2333984375 \n",
      "train_step 577,\ttest_acc:0.9218999743461609,\ttest_loss:2759.11767578125 \n",
      "train_step 578,\ttest_acc:0.9228000044822693,\ttest_loss:2801.19384765625 \n",
      "train_step 579,\ttest_acc:0.9207000136375427,\ttest_loss:2833.26416015625 \n",
      "train_step 580,\ttest_acc:0.9143999814987183,\ttest_loss:3070.323486328125 \n",
      "train_step 581,\ttest_acc:0.9016000032424927,\ttest_loss:3297.973876953125 \n",
      "train_step 582,\ttest_acc:0.9226999878883362,\ttest_loss:2778.9013671875 \n",
      "train_step 583,\ttest_acc:0.9193000197410583,\ttest_loss:2866.59716796875 \n",
      "train_step 584,\ttest_acc:0.9225000143051147,\ttest_loss:2782.39794921875 \n",
      "train_step 585,\ttest_acc:0.9208999872207642,\ttest_loss:2873.07861328125 \n",
      "train_step 586,\ttest_acc:0.9143000245094299,\ttest_loss:3017.34033203125 \n",
      "train_step 587,\ttest_acc:0.9196000099182129,\ttest_loss:2875.838134765625 \n",
      "train_step 588,\ttest_acc:0.9204000234603882,\ttest_loss:2854.974609375 \n",
      "train_step 589,\ttest_acc:0.920199990272522,\ttest_loss:2841.92578125 \n",
      "train_step 590,\ttest_acc:0.9203000068664551,\ttest_loss:2906.90625 \n",
      "train_step 591,\ttest_acc:0.9190999865531921,\ttest_loss:2877.17724609375 \n",
      "train_step 592,\ttest_acc:0.9222000241279602,\ttest_loss:2823.53955078125 \n",
      "train_step 593,\ttest_acc:0.9212999939918518,\ttest_loss:2870.68408203125 \n",
      "train_step 594,\ttest_acc:0.9122999906539917,\ttest_loss:3046.58544921875 \n",
      "train_step 595,\ttest_acc:0.9107999801635742,\ttest_loss:3068.657470703125 \n",
      "train_step 596,\ttest_acc:0.9190999865531921,\ttest_loss:2940.69580078125 \n",
      "train_step 597,\ttest_acc:0.9174000024795532,\ttest_loss:2926.05419921875 \n",
      "train_step 598,\ttest_acc:0.9240000247955322,\ttest_loss:2793.638671875 \n",
      "train_step 599,\ttest_acc:0.9193000197410583,\ttest_loss:2951.927734375 \n",
      "train_step 600,\ttest_acc:0.9077000021934509,\ttest_loss:3191.11572265625 \n",
      "train_step 601,\ttest_acc:0.9196000099182129,\ttest_loss:2875.07568359375 \n",
      "train_step 602,\ttest_acc:0.9236999750137329,\ttest_loss:2823.58251953125 \n",
      "train_step 603,\ttest_acc:0.9236999750137329,\ttest_loss:2789.62353515625 \n",
      "train_step 604,\ttest_acc:0.9210000038146973,\ttest_loss:2809.698486328125 \n",
      "train_step 605,\ttest_acc:0.9211999773979187,\ttest_loss:2831.97021484375 \n",
      "train_step 606,\ttest_acc:0.9199000000953674,\ttest_loss:2864.76318359375 \n",
      "train_step 607,\ttest_acc:0.9150000214576721,\ttest_loss:3060.3544921875 \n",
      "train_step 608,\ttest_acc:0.9190000295639038,\ttest_loss:2964.62890625 \n",
      "train_step 609,\ttest_acc:0.9171000123023987,\ttest_loss:2940.423828125 \n",
      "train_step 610,\ttest_acc:0.9200999736785889,\ttest_loss:2861.7763671875 \n",
      "train_step 611,\ttest_acc:0.9222999811172485,\ttest_loss:2809.9326171875 \n",
      "train_step 612,\ttest_acc:0.921500027179718,\ttest_loss:2879.7763671875 \n",
      "train_step 613,\ttest_acc:0.9138000011444092,\ttest_loss:3065.20166015625 \n",
      "train_step 614,\ttest_acc:0.9199000000953674,\ttest_loss:2843.65234375 \n",
      "train_step 615,\ttest_acc:0.9214000105857849,\ttest_loss:2802.268310546875 \n",
      "train_step 616,\ttest_acc:0.9200999736785889,\ttest_loss:2874.56787109375 \n",
      "train_step 617,\ttest_acc:0.9218999743461609,\ttest_loss:2798.19677734375 \n",
      "train_step 618,\ttest_acc:0.9171000123023987,\ttest_loss:2968.72412109375 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_step 619,\ttest_acc:0.9207000136375427,\ttest_loss:2839.827392578125 \n",
      "train_step 620,\ttest_acc:0.9200999736785889,\ttest_loss:2876.2724609375 \n",
      "train_step 621,\ttest_acc:0.9199000000953674,\ttest_loss:2849.6337890625 \n",
      "train_step 622,\ttest_acc:0.9154999852180481,\ttest_loss:3080.18505859375 \n",
      "train_step 623,\ttest_acc:0.9139999747276306,\ttest_loss:3060.361328125 \n",
      "train_step 624,\ttest_acc:0.911300003528595,\ttest_loss:3117.9716796875 \n",
      "train_step 625,\ttest_acc:0.9203000068664551,\ttest_loss:2882.91943359375 \n",
      "train_step 626,\ttest_acc:0.9225999712944031,\ttest_loss:2814.58349609375 \n",
      "train_step 627,\ttest_acc:0.9133999943733215,\ttest_loss:3017.111572265625 \n",
      "train_step 628,\ttest_acc:0.9089000225067139,\ttest_loss:3185.8671875 \n",
      "train_step 629,\ttest_acc:0.9017999768257141,\ttest_loss:3302.723388671875 \n",
      "train_step 630,\ttest_acc:0.9139999747276306,\ttest_loss:2994.320068359375 \n",
      "train_step 631,\ttest_acc:0.9161999821662903,\ttest_loss:2954.178955078125 \n",
      "train_step 632,\ttest_acc:0.91839998960495,\ttest_loss:2898.53369140625 \n",
      "train_step 633,\ttest_acc:0.9211000204086304,\ttest_loss:2799.729248046875 \n",
      "train_step 634,\ttest_acc:0.9205999970436096,\ttest_loss:2796.45849609375 \n",
      "train_step 635,\ttest_acc:0.9211000204086304,\ttest_loss:2845.883544921875 \n",
      "train_step 636,\ttest_acc:0.9161999821662903,\ttest_loss:2961.31103515625 \n",
      "train_step 637,\ttest_acc:0.9160000085830688,\ttest_loss:2957.67431640625 \n",
      "train_step 638,\ttest_acc:0.9082000255584717,\ttest_loss:3177.74951171875 \n",
      "train_step 639,\ttest_acc:0.9215999841690063,\ttest_loss:2798.4912109375 \n",
      "train_step 640,\ttest_acc:0.916700005531311,\ttest_loss:2891.76611328125 \n",
      "train_step 641,\ttest_acc:0.9207000136375427,\ttest_loss:2885.236328125 \n",
      "train_step 642,\ttest_acc:0.9189000129699707,\ttest_loss:2856.6865234375 \n",
      "train_step 643,\ttest_acc:0.9193000197410583,\ttest_loss:2844.016357421875 \n",
      "train_step 644,\ttest_acc:0.9204999804496765,\ttest_loss:2800.61572265625 \n",
      "train_step 645,\ttest_acc:0.9185000061988831,\ttest_loss:2842.07568359375 \n",
      "train_step 646,\ttest_acc:0.916700005531311,\ttest_loss:3027.121337890625 \n",
      "train_step 647,\ttest_acc:0.9121999740600586,\ttest_loss:3024.256591796875 \n",
      "train_step 648,\ttest_acc:0.9171000123023987,\ttest_loss:2894.19287109375 \n",
      "train_step 649,\ttest_acc:0.9114999771118164,\ttest_loss:3015.234619140625 \n",
      "train_step 650,\ttest_acc:0.9225000143051147,\ttest_loss:2820.2763671875 \n",
      "train_step 651,\ttest_acc:0.921999990940094,\ttest_loss:2823.419189453125 \n",
      "train_step 652,\ttest_acc:0.9215999841690063,\ttest_loss:2810.83447265625 \n",
      "train_step 653,\ttest_acc:0.9211000204086304,\ttest_loss:2868.8349609375 \n",
      "train_step 654,\ttest_acc:0.9200000166893005,\ttest_loss:2899.231201171875 \n",
      "train_step 655,\ttest_acc:0.9189000129699707,\ttest_loss:2884.400634765625 \n",
      "train_step 656,\ttest_acc:0.9180999994277954,\ttest_loss:2898.82177734375 \n",
      "train_step 657,\ttest_acc:0.9090999960899353,\ttest_loss:3220.78369140625 \n",
      "train_step 658,\ttest_acc:0.9132999777793884,\ttest_loss:3102.4189453125 \n",
      "train_step 659,\ttest_acc:0.9190000295639038,\ttest_loss:2934.00244140625 \n",
      "train_step 660,\ttest_acc:0.9133999943733215,\ttest_loss:2998.28662109375 \n",
      "train_step 661,\ttest_acc:0.9190999865531921,\ttest_loss:2918.74853515625 \n",
      "train_step 662,\ttest_acc:0.9212999939918518,\ttest_loss:2837.5185546875 \n",
      "train_step 663,\ttest_acc:0.9174000024795532,\ttest_loss:2952.49365234375 \n",
      "train_step 664,\ttest_acc:0.920799970626831,\ttest_loss:2805.328857421875 \n",
      "train_step 665,\ttest_acc:0.9208999872207642,\ttest_loss:2854.69580078125 \n",
      "train_step 666,\ttest_acc:0.9189000129699707,\ttest_loss:2851.576171875 \n",
      "train_step 667,\ttest_acc:0.9235000014305115,\ttest_loss:2761.78271484375 \n",
      "train_step 668,\ttest_acc:0.9200999736785889,\ttest_loss:2892.98486328125 \n",
      "train_step 669,\ttest_acc:0.9204000234603882,\ttest_loss:2886.535400390625 \n",
      "train_step 670,\ttest_acc:0.9204000234603882,\ttest_loss:2855.21337890625 \n",
      "train_step 671,\ttest_acc:0.9203000068664551,\ttest_loss:2840.286376953125 \n",
      "train_step 672,\ttest_acc:0.9207000136375427,\ttest_loss:2866.258544921875 \n",
      "train_step 673,\ttest_acc:0.9196000099182129,\ttest_loss:2925.94482421875 \n",
      "train_step 674,\ttest_acc:0.9204999804496765,\ttest_loss:2877.08740234375 \n",
      "train_step 675,\ttest_acc:0.9217000007629395,\ttest_loss:2831.919677734375 \n",
      "train_step 676,\ttest_acc:0.9218999743461609,\ttest_loss:2803.8974609375 \n",
      "train_step 677,\ttest_acc:0.913100004196167,\ttest_loss:3136.2900390625 \n",
      "train_step 678,\ttest_acc:0.9172999858856201,\ttest_loss:2914.766845703125 \n",
      "train_step 679,\ttest_acc:0.921999990940094,\ttest_loss:2841.152587890625 \n",
      "train_step 680,\ttest_acc:0.9175999760627747,\ttest_loss:2940.0380859375 \n",
      "train_step 681,\ttest_acc:0.9169999957084656,\ttest_loss:2962.28173828125 \n",
      "train_step 682,\ttest_acc:0.9185000061988831,\ttest_loss:2991.611328125 \n",
      "train_step 683,\ttest_acc:0.9208999872207642,\ttest_loss:2889.650634765625 \n",
      "train_step 684,\ttest_acc:0.9212999939918518,\ttest_loss:2853.62109375 \n",
      "train_step 685,\ttest_acc:0.9200000166893005,\ttest_loss:2892.287109375 \n",
      "train_step 686,\ttest_acc:0.9222999811172485,\ttest_loss:2850.11865234375 \n",
      "train_step 687,\ttest_acc:0.9114999771118164,\ttest_loss:3073.0400390625 \n",
      "train_step 688,\ttest_acc:0.9068999886512756,\ttest_loss:3208.50341796875 \n",
      "train_step 689,\ttest_acc:0.9150999784469604,\ttest_loss:3011.83447265625 \n",
      "train_step 690,\ttest_acc:0.9186000227928162,\ttest_loss:2945.1279296875 \n",
      "train_step 691,\ttest_acc:0.9143000245094299,\ttest_loss:2961.39306640625 \n",
      "train_step 692,\ttest_acc:0.9165999889373779,\ttest_loss:2996.053466796875 \n",
      "train_step 693,\ttest_acc:0.9122999906539917,\ttest_loss:3115.907470703125 \n",
      "train_step 694,\ttest_acc:0.9204000234603882,\ttest_loss:2838.638427734375 \n",
      "train_step 695,\ttest_acc:0.9154000282287598,\ttest_loss:2947.99853515625 \n",
      "train_step 696,\ttest_acc:0.9175999760627747,\ttest_loss:2927.808837890625 \n",
      "train_step 697,\ttest_acc:0.921500027179718,\ttest_loss:2872.35302734375 \n",
      "train_step 698,\ttest_acc:0.9135000109672546,\ttest_loss:3033.78466796875 \n",
      "train_step 699,\ttest_acc:0.9204000234603882,\ttest_loss:2913.898193359375 \n",
      "train_step 700,\ttest_acc:0.9162999987602234,\ttest_loss:2927.22509765625 \n",
      "train_step 701,\ttest_acc:0.9215999841690063,\ttest_loss:2865.571044921875 \n",
      "train_step 702,\ttest_acc:0.9000999927520752,\ttest_loss:3340.349609375 \n",
      "train_step 703,\ttest_acc:0.9124000072479248,\ttest_loss:3072.7958984375 \n",
      "train_step 704,\ttest_acc:0.913100004196167,\ttest_loss:3040.2998046875 \n",
      "train_step 705,\ttest_acc:0.9172000288963318,\ttest_loss:2982.80224609375 \n",
      "train_step 706,\ttest_acc:0.920799970626831,\ttest_loss:2863.10986328125 \n",
      "train_step 707,\ttest_acc:0.9193000197410583,\ttest_loss:2836.344482421875 \n",
      "train_step 708,\ttest_acc:0.9142000079154968,\ttest_loss:3049.49609375 \n",
      "train_step 709,\ttest_acc:0.9125999808311462,\ttest_loss:3024.433837890625 \n",
      "train_step 710,\ttest_acc:0.9193000197410583,\ttest_loss:2857.169921875 \n",
      "train_step 711,\ttest_acc:0.9194999933242798,\ttest_loss:2883.08447265625 \n",
      "train_step 712,\ttest_acc:0.9197999835014343,\ttest_loss:2886.6533203125 \n",
      "train_step 713,\ttest_acc:0.9186000227928162,\ttest_loss:2891.960693359375 \n",
      "train_step 714,\ttest_acc:0.9146999716758728,\ttest_loss:2983.470458984375 \n",
      "train_step 715,\ttest_acc:0.9197999835014343,\ttest_loss:2874.599853515625 \n",
      "train_step 716,\ttest_acc:0.9100000262260437,\ttest_loss:3199.17822265625 \n",
      "train_step 717,\ttest_acc:0.9165999889373779,\ttest_loss:3012.294677734375 \n",
      "train_step 718,\ttest_acc:0.9165999889373779,\ttest_loss:2985.589111328125 \n",
      "train_step 719,\ttest_acc:0.9164000153541565,\ttest_loss:2962.11669921875 \n",
      "train_step 720,\ttest_acc:0.9150000214576721,\ttest_loss:3132.24560546875 \n",
      "train_step 721,\ttest_acc:0.9197999835014343,\ttest_loss:2905.098388671875 \n",
      "train_step 722,\ttest_acc:0.9157000184059143,\ttest_loss:3067.76611328125 \n",
      "train_step 723,\ttest_acc:0.9157000184059143,\ttest_loss:3043.494140625 \n",
      "train_step 724,\ttest_acc:0.9139000177383423,\ttest_loss:3003.607666015625 \n",
      "train_step 725,\ttest_acc:0.9158999919891357,\ttest_loss:2925.111328125 \n",
      "train_step 726,\ttest_acc:0.9204999804496765,\ttest_loss:2847.90478515625 \n",
      "train_step 727,\ttest_acc:0.9204000234603882,\ttest_loss:2933.5859375 \n",
      "train_step 728,\ttest_acc:0.911899983882904,\ttest_loss:3083.538330078125 \n",
      "train_step 729,\ttest_acc:0.9211000204086304,\ttest_loss:2904.59814453125 \n",
      "train_step 730,\ttest_acc:0.9135000109672546,\ttest_loss:3027.6630859375 \n",
      "train_step 731,\ttest_acc:0.9203000068664551,\ttest_loss:2932.193359375 \n",
      "train_step 732,\ttest_acc:0.9225000143051147,\ttest_loss:2805.37353515625 \n",
      "train_step 733,\ttest_acc:0.9204000234603882,\ttest_loss:2882.32861328125 \n",
      "train_step 734,\ttest_acc:0.9186000227928162,\ttest_loss:2893.77734375 \n",
      "train_step 735,\ttest_acc:0.9164000153541565,\ttest_loss:2953.3330078125 \n",
      "train_step 736,\ttest_acc:0.9085999727249146,\ttest_loss:3285.02099609375 \n",
      "train_step 737,\ttest_acc:0.9100000262260437,\ttest_loss:3153.9921875 \n",
      "train_step 738,\ttest_acc:0.9169999957084656,\ttest_loss:3030.1728515625 \n",
      "train_step 739,\ttest_acc:0.9176999926567078,\ttest_loss:2979.64892578125 \n",
      "train_step 740,\ttest_acc:0.9126999974250793,\ttest_loss:3173.79248046875 \n",
      "train_step 741,\ttest_acc:0.917900025844574,\ttest_loss:2873.43505859375 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_step 742,\ttest_acc:0.9168999791145325,\ttest_loss:2974.19189453125 \n",
      "train_step 743,\ttest_acc:0.9180999994277954,\ttest_loss:2980.74853515625 \n",
      "train_step 744,\ttest_acc:0.9110999703407288,\ttest_loss:3114.195556640625 \n",
      "train_step 745,\ttest_acc:0.9154999852180481,\ttest_loss:3044.3369140625 \n",
      "train_step 746,\ttest_acc:0.9168999791145325,\ttest_loss:2964.158935546875 \n",
      "train_step 747,\ttest_acc:0.9114999771118164,\ttest_loss:3130.852294921875 \n",
      "train_step 748,\ttest_acc:0.9157999753952026,\ttest_loss:2986.108154296875 \n",
      "train_step 749,\ttest_acc:0.9161999821662903,\ttest_loss:2878.71875 \n",
      "train_step 750,\ttest_acc:0.9210000038146973,\ttest_loss:2849.182373046875 \n",
      "train_step 751,\ttest_acc:0.9045000076293945,\ttest_loss:3398.35791015625 \n",
      "train_step 752,\ttest_acc:0.9122999906539917,\ttest_loss:3168.61083984375 \n",
      "train_step 753,\ttest_acc:0.9179999828338623,\ttest_loss:2941.4287109375 \n",
      "train_step 754,\ttest_acc:0.9164000153541565,\ttest_loss:2988.40234375 \n",
      "train_step 755,\ttest_acc:0.917900025844574,\ttest_loss:2957.5595703125 \n",
      "train_step 756,\ttest_acc:0.9146000146865845,\ttest_loss:2971.013916015625 \n",
      "train_step 757,\ttest_acc:0.9175999760627747,\ttest_loss:2905.91552734375 \n",
      "train_step 758,\ttest_acc:0.9211999773979187,\ttest_loss:2842.97119140625 \n",
      "train_step 759,\ttest_acc:0.9193999767303467,\ttest_loss:2892.38818359375 \n",
      "train_step 760,\ttest_acc:0.9164000153541565,\ttest_loss:2959.6103515625 \n",
      "train_step 761,\ttest_acc:0.9196000099182129,\ttest_loss:2934.43896484375 \n",
      "train_step 762,\ttest_acc:0.9128999710083008,\ttest_loss:3025.94091796875 \n",
      "train_step 763,\ttest_acc:0.913100004196167,\ttest_loss:2982.000244140625 \n",
      "train_step 764,\ttest_acc:0.9124000072479248,\ttest_loss:3041.834228515625 \n",
      "train_step 765,\ttest_acc:0.9072999954223633,\ttest_loss:3164.087890625 \n",
      "train_step 766,\ttest_acc:0.9035000205039978,\ttest_loss:3282.0517578125 \n",
      "train_step 767,\ttest_acc:0.9150999784469604,\ttest_loss:3002.93701171875 \n",
      "train_step 768,\ttest_acc:0.9176999926567078,\ttest_loss:2946.29248046875 \n",
      "train_step 769,\ttest_acc:0.9143000245094299,\ttest_loss:3043.86328125 \n",
      "train_step 770,\ttest_acc:0.9169999957084656,\ttest_loss:2972.78125 \n",
      "train_step 771,\ttest_acc:0.9147999882698059,\ttest_loss:3035.094970703125 \n",
      "train_step 772,\ttest_acc:0.9179999828338623,\ttest_loss:2883.82568359375 \n",
      "train_step 773,\ttest_acc:0.9182999730110168,\ttest_loss:2956.054443359375 \n",
      "train_step 774,\ttest_acc:0.9171000123023987,\ttest_loss:2949.14501953125 \n",
      "train_step 775,\ttest_acc:0.9072999954223633,\ttest_loss:3207.56201171875 \n",
      "train_step 776,\ttest_acc:0.9175999760627747,\ttest_loss:2925.510009765625 \n",
      "train_step 777,\ttest_acc:0.9064000248908997,\ttest_loss:3171.792724609375 \n",
      "train_step 778,\ttest_acc:0.9160000085830688,\ttest_loss:2994.91064453125 \n",
      "train_step 779,\ttest_acc:0.9180999994277954,\ttest_loss:2919.78173828125 \n",
      "train_step 780,\ttest_acc:0.9122999906539917,\ttest_loss:3072.737548828125 \n",
      "train_step 781,\ttest_acc:0.9086999893188477,\ttest_loss:3097.271484375 \n",
      "train_step 782,\ttest_acc:0.920199990272522,\ttest_loss:2876.32568359375 \n",
      "train_step 783,\ttest_acc:0.9161999821662903,\ttest_loss:3045.239990234375 \n",
      "train_step 784,\ttest_acc:0.9193000197410583,\ttest_loss:2846.40234375 \n",
      "train_step 785,\ttest_acc:0.8988999724388123,\ttest_loss:3353.13818359375 \n",
      "train_step 786,\ttest_acc:0.9179999828338623,\ttest_loss:2886.307373046875 \n",
      "train_step 787,\ttest_acc:0.9162999987602234,\ttest_loss:2981.11767578125 \n",
      "train_step 788,\ttest_acc:0.9193999767303467,\ttest_loss:2877.435546875 \n",
      "train_step 789,\ttest_acc:0.9176999926567078,\ttest_loss:2867.7216796875 \n",
      "train_step 790,\ttest_acc:0.9215999841690063,\ttest_loss:2863.1474609375 \n",
      "train_step 791,\ttest_acc:0.9054999947547913,\ttest_loss:3190.38232421875 \n",
      "train_step 792,\ttest_acc:0.907800018787384,\ttest_loss:3157.3046875 \n",
      "train_step 793,\ttest_acc:0.9121999740600586,\ttest_loss:3061.15380859375 \n",
      "train_step 794,\ttest_acc:0.9190000295639038,\ttest_loss:2889.52001953125 \n",
      "train_step 795,\ttest_acc:0.9193000197410583,\ttest_loss:2937.63427734375 \n",
      "train_step 796,\ttest_acc:0.9174000024795532,\ttest_loss:2920.75537109375 \n",
      "train_step 797,\ttest_acc:0.9197999835014343,\ttest_loss:2892.841796875 \n",
      "train_step 798,\ttest_acc:0.9196000099182129,\ttest_loss:2879.240478515625 \n",
      "train_step 799,\ttest_acc:0.9132999777793884,\ttest_loss:3061.6669921875 \n",
      "train_step 800,\ttest_acc:0.8973000049591064,\ttest_loss:3624.4375 \n",
      "train_step 801,\ttest_acc:0.9157999753952026,\ttest_loss:3004.395263671875 \n",
      "train_step 802,\ttest_acc:0.9169999957084656,\ttest_loss:2920.327880859375 \n",
      "train_step 803,\ttest_acc:0.9147999882698059,\ttest_loss:3037.8984375 \n",
      "train_step 804,\ttest_acc:0.9121000170707703,\ttest_loss:3049.887451171875 \n",
      "train_step 805,\ttest_acc:0.9146999716758728,\ttest_loss:3053.175048828125 \n",
      "train_step 806,\ttest_acc:0.914900004863739,\ttest_loss:3024.23388671875 \n",
      "train_step 807,\ttest_acc:0.9200000166893005,\ttest_loss:2905.8115234375 \n",
      "train_step 808,\ttest_acc:0.9204999804496765,\ttest_loss:2852.873779296875 \n",
      "train_step 809,\ttest_acc:0.9169999957084656,\ttest_loss:2911.16259765625 \n",
      "train_step 810,\ttest_acc:0.9162999987602234,\ttest_loss:2923.72265625 \n",
      "train_step 811,\ttest_acc:0.9146999716758728,\ttest_loss:3088.74267578125 \n",
      "train_step 812,\ttest_acc:0.9172000288963318,\ttest_loss:2920.07177734375 \n",
      "train_step 813,\ttest_acc:0.9136000275611877,\ttest_loss:3049.43212890625 \n",
      "train_step 814,\ttest_acc:0.9176999926567078,\ttest_loss:2996.48388671875 \n",
      "train_step 815,\ttest_acc:0.9205999970436096,\ttest_loss:2862.78369140625 \n",
      "train_step 816,\ttest_acc:0.9210000038146973,\ttest_loss:2879.26708984375 \n",
      "train_step 817,\ttest_acc:0.9162999987602234,\ttest_loss:2968.721923828125 \n",
      "train_step 818,\ttest_acc:0.9199000000953674,\ttest_loss:2915.827392578125 \n",
      "train_step 819,\ttest_acc:0.916100025177002,\ttest_loss:2966.080078125 \n",
      "train_step 820,\ttest_acc:0.9161999821662903,\ttest_loss:3072.248046875 \n",
      "train_step 821,\ttest_acc:0.919700026512146,\ttest_loss:2974.1220703125 \n",
      "train_step 822,\ttest_acc:0.9197999835014343,\ttest_loss:2950.015625 \n",
      "train_step 823,\ttest_acc:0.9208999872207642,\ttest_loss:2893.806396484375 \n",
      "train_step 824,\ttest_acc:0.9218000173568726,\ttest_loss:2956.345947265625 \n",
      "train_step 825,\ttest_acc:0.9150999784469604,\ttest_loss:3041.8134765625 \n",
      "train_step 826,\ttest_acc:0.9180999994277954,\ttest_loss:3030.3037109375 \n",
      "train_step 827,\ttest_acc:0.9061999917030334,\ttest_loss:3244.3125 \n",
      "train_step 828,\ttest_acc:0.9232000112533569,\ttest_loss:2881.5859375 \n",
      "train_step 829,\ttest_acc:0.9211999773979187,\ttest_loss:2865.76123046875 \n",
      "train_step 830,\ttest_acc:0.9222000241279602,\ttest_loss:2928.63037109375 \n",
      "train_step 831,\ttest_acc:0.9171000123023987,\ttest_loss:3083.525390625 \n",
      "train_step 832,\ttest_acc:0.9164000153541565,\ttest_loss:2980.2177734375 \n",
      "train_step 833,\ttest_acc:0.9218999743461609,\ttest_loss:2885.28173828125 \n",
      "train_step 834,\ttest_acc:0.9247999787330627,\ttest_loss:2789.541259765625 \n",
      "train_step 835,\ttest_acc:0.920799970626831,\ttest_loss:2890.099609375 \n",
      "train_step 836,\ttest_acc:0.92330002784729,\ttest_loss:2842.48388671875 \n",
      "train_step 837,\ttest_acc:0.9229999780654907,\ttest_loss:2852.206787109375 \n",
      "train_step 838,\ttest_acc:0.9182000160217285,\ttest_loss:2911.03955078125 \n",
      "train_step 839,\ttest_acc:0.9193999767303467,\ttest_loss:2849.1025390625 \n",
      "train_step 840,\ttest_acc:0.9140999913215637,\ttest_loss:2976.56298828125 \n",
      "train_step 841,\ttest_acc:0.9194999933242798,\ttest_loss:2816.015625 \n",
      "train_step 842,\ttest_acc:0.9207000136375427,\ttest_loss:2806.630859375 \n",
      "train_step 843,\ttest_acc:0.9221000075340271,\ttest_loss:2770.47509765625 \n",
      "train_step 844,\ttest_acc:0.9218000173568726,\ttest_loss:2791.302490234375 \n",
      "train_step 845,\ttest_acc:0.921500027179718,\ttest_loss:2818.107177734375 \n",
      "train_step 846,\ttest_acc:0.9186999797821045,\ttest_loss:2929.3828125 \n",
      "train_step 847,\ttest_acc:0.9156000018119812,\ttest_loss:3009.82958984375 \n",
      "train_step 848,\ttest_acc:0.9128999710083008,\ttest_loss:3090.947509765625 \n",
      "train_step 849,\ttest_acc:0.9146999716758728,\ttest_loss:3030.9365234375 \n",
      "train_step 850,\ttest_acc:0.9151999950408936,\ttest_loss:2999.761474609375 \n",
      "train_step 851,\ttest_acc:0.9154000282287598,\ttest_loss:3023.65625 \n",
      "train_step 852,\ttest_acc:0.9235000014305115,\ttest_loss:2842.281494140625 \n",
      "train_step 853,\ttest_acc:0.9259999990463257,\ttest_loss:2727.50244140625 \n",
      "train_step 854,\ttest_acc:0.9264000058174133,\ttest_loss:2743.565185546875 \n",
      "train_step 855,\ttest_acc:0.9168000221252441,\ttest_loss:2961.017578125 \n",
      "train_step 856,\ttest_acc:0.9204000234603882,\ttest_loss:2876.32177734375 \n",
      "train_step 857,\ttest_acc:0.9211000204086304,\ttest_loss:2815.676025390625 \n",
      "train_step 858,\ttest_acc:0.9244999885559082,\ttest_loss:2746.512451171875 \n",
      "train_step 859,\ttest_acc:0.9204999804496765,\ttest_loss:2846.1806640625 \n",
      "train_step 860,\ttest_acc:0.9217000007629395,\ttest_loss:2884.112548828125 \n",
      "train_step 861,\ttest_acc:0.921500027179718,\ttest_loss:2877.712158203125 \n",
      "train_step 862,\ttest_acc:0.9197999835014343,\ttest_loss:2832.41455078125 \n",
      "train_step 863,\ttest_acc:0.9215999841690063,\ttest_loss:2826.968505859375 \n",
      "train_step 864,\ttest_acc:0.9039999842643738,\ttest_loss:3319.154541015625 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_step 865,\ttest_acc:0.9208999872207642,\ttest_loss:2868.10400390625 \n",
      "train_step 866,\ttest_acc:0.9158999919891357,\ttest_loss:3013.41943359375 \n",
      "train_step 867,\ttest_acc:0.9186000227928162,\ttest_loss:2938.175048828125 \n",
      "train_step 868,\ttest_acc:0.9178000092506409,\ttest_loss:2928.005615234375 \n",
      "train_step 869,\ttest_acc:0.913100004196167,\ttest_loss:3097.5693359375 \n",
      "train_step 870,\ttest_acc:0.9204999804496765,\ttest_loss:2919.097900390625 \n",
      "train_step 871,\ttest_acc:0.9160000085830688,\ttest_loss:2990.357421875 \n",
      "train_step 872,\ttest_acc:0.9218999743461609,\ttest_loss:2861.7587890625 \n",
      "train_step 873,\ttest_acc:0.9147999882698059,\ttest_loss:2996.532470703125 \n",
      "train_step 874,\ttest_acc:0.9179999828338623,\ttest_loss:2895.5673828125 \n",
      "train_step 875,\ttest_acc:0.9187999963760376,\ttest_loss:2935.759765625 \n",
      "train_step 876,\ttest_acc:0.9156000018119812,\ttest_loss:3011.493408203125 \n",
      "train_step 877,\ttest_acc:0.9024999737739563,\ttest_loss:3552.2099609375 \n",
      "train_step 878,\ttest_acc:0.9144999980926514,\ttest_loss:2995.93603515625 \n",
      "train_step 879,\ttest_acc:0.9154999852180481,\ttest_loss:2992.131103515625 \n",
      "train_step 880,\ttest_acc:0.9233999848365784,\ttest_loss:2820.87255859375 \n",
      "train_step 881,\ttest_acc:0.920799970626831,\ttest_loss:2847.575439453125 \n",
      "train_step 882,\ttest_acc:0.9200000166893005,\ttest_loss:2880.1650390625 \n",
      "train_step 883,\ttest_acc:0.9229000210762024,\ttest_loss:2926.49169921875 \n",
      "train_step 884,\ttest_acc:0.8952999711036682,\ttest_loss:3725.322509765625 \n",
      "train_step 885,\ttest_acc:0.9154999852180481,\ttest_loss:3009.71533203125 \n",
      "train_step 886,\ttest_acc:0.9196000099182129,\ttest_loss:2931.580810546875 \n",
      "train_step 887,\ttest_acc:0.9133999943733215,\ttest_loss:3173.817626953125 \n",
      "train_step 888,\ttest_acc:0.9160000085830688,\ttest_loss:2950.318115234375 \n",
      "train_step 889,\ttest_acc:0.9171000123023987,\ttest_loss:2944.431884765625 \n",
      "train_step 890,\ttest_acc:0.9133999943733215,\ttest_loss:3083.748046875 \n",
      "train_step 891,\ttest_acc:0.9212999939918518,\ttest_loss:2858.14697265625 \n",
      "train_step 892,\ttest_acc:0.9114999771118164,\ttest_loss:3077.727783203125 \n",
      "train_step 893,\ttest_acc:0.9182000160217285,\ttest_loss:2892.23974609375 \n",
      "train_step 894,\ttest_acc:0.9208999872207642,\ttest_loss:2872.034423828125 \n",
      "train_step 895,\ttest_acc:0.9203000068664551,\ttest_loss:2864.343994140625 \n",
      "train_step 896,\ttest_acc:0.9225000143051147,\ttest_loss:2798.7607421875 \n",
      "train_step 897,\ttest_acc:0.9161999821662903,\ttest_loss:2953.83642578125 \n",
      "train_step 898,\ttest_acc:0.9151999950408936,\ttest_loss:3009.40576171875 \n",
      "train_step 899,\ttest_acc:0.9171000123023987,\ttest_loss:2922.96484375 \n",
      "train_step 900,\ttest_acc:0.9200999736785889,\ttest_loss:2833.65234375 \n",
      "train_step 901,\ttest_acc:0.9243000149726868,\ttest_loss:2791.00341796875 \n",
      "train_step 902,\ttest_acc:0.9014000296592712,\ttest_loss:3501.328125 \n",
      "train_step 903,\ttest_acc:0.9182999730110168,\ttest_loss:2980.568603515625 \n",
      "train_step 904,\ttest_acc:0.91839998960495,\ttest_loss:2921.35546875 \n",
      "train_step 905,\ttest_acc:0.9214000105857849,\ttest_loss:2852.083251953125 \n",
      "train_step 906,\ttest_acc:0.9199000000953674,\ttest_loss:2909.23095703125 \n",
      "train_step 907,\ttest_acc:0.9093000292778015,\ttest_loss:3186.99755859375 \n",
      "train_step 908,\ttest_acc:0.9218999743461609,\ttest_loss:2812.12158203125 \n",
      "train_step 909,\ttest_acc:0.9215999841690063,\ttest_loss:2851.4140625 \n",
      "train_step 910,\ttest_acc:0.9118000268936157,\ttest_loss:3041.499755859375 \n",
      "train_step 911,\ttest_acc:0.9196000099182129,\ttest_loss:2958.33251953125 \n",
      "train_step 912,\ttest_acc:0.9125999808311462,\ttest_loss:3087.27783203125 \n",
      "train_step 913,\ttest_acc:0.9190000295639038,\ttest_loss:2948.97021484375 \n",
      "train_step 914,\ttest_acc:0.919700026512146,\ttest_loss:2989.5791015625 \n",
      "train_step 915,\ttest_acc:0.9229000210762024,\ttest_loss:2798.681640625 \n",
      "train_step 916,\ttest_acc:0.9241999983787537,\ttest_loss:2775.4921875 \n",
      "train_step 917,\ttest_acc:0.9251000285148621,\ttest_loss:2775.2900390625 \n",
      "train_step 918,\ttest_acc:0.9218999743461609,\ttest_loss:2849.98974609375 \n",
      "train_step 919,\ttest_acc:0.9204999804496765,\ttest_loss:2900.19580078125 \n",
      "train_step 920,\ttest_acc:0.9192000031471252,\ttest_loss:2924.64404296875 \n",
      "train_step 921,\ttest_acc:0.9208999872207642,\ttest_loss:2804.06201171875 \n",
      "train_step 922,\ttest_acc:0.9222999811172485,\ttest_loss:2808.7177734375 \n",
      "train_step 923,\ttest_acc:0.9190999865531921,\ttest_loss:2851.29345703125 \n",
      "train_step 924,\ttest_acc:0.920799970626831,\ttest_loss:2866.46533203125 \n",
      "train_step 925,\ttest_acc:0.9211000204086304,\ttest_loss:2829.57470703125 \n",
      "train_step 926,\ttest_acc:0.9179999828338623,\ttest_loss:2916.80419921875 \n",
      "train_step 927,\ttest_acc:0.9204000234603882,\ttest_loss:2896.462646484375 \n",
      "train_step 928,\ttest_acc:0.9151999950408936,\ttest_loss:3079.916015625 \n",
      "train_step 929,\ttest_acc:0.9190000295639038,\ttest_loss:3006.77685546875 \n",
      "train_step 930,\ttest_acc:0.8970999717712402,\ttest_loss:3610.5107421875 \n",
      "train_step 931,\ttest_acc:0.9229999780654907,\ttest_loss:2840.154296875 \n",
      "train_step 932,\ttest_acc:0.9235000014305115,\ttest_loss:2856.03271484375 \n",
      "train_step 933,\ttest_acc:0.9218999743461609,\ttest_loss:2867.90966796875 \n",
      "train_step 934,\ttest_acc:0.9200000166893005,\ttest_loss:2920.191650390625 \n",
      "train_step 935,\ttest_acc:0.9200000166893005,\ttest_loss:2976.84375 \n",
      "train_step 936,\ttest_acc:0.9194999933242798,\ttest_loss:2971.72998046875 \n",
      "train_step 937,\ttest_acc:0.9138000011444092,\ttest_loss:3127.62060546875 \n",
      "train_step 938,\ttest_acc:0.919700026512146,\ttest_loss:2947.978515625 \n",
      "train_step 939,\ttest_acc:0.9210000038146973,\ttest_loss:2861.215576171875 \n",
      "train_step 940,\ttest_acc:0.9223999977111816,\ttest_loss:2815.75537109375 \n",
      "train_step 941,\ttest_acc:0.9208999872207642,\ttest_loss:2881.18701171875 \n",
      "train_step 942,\ttest_acc:0.9218000173568726,\ttest_loss:2879.26025390625 \n",
      "train_step 943,\ttest_acc:0.9031000137329102,\ttest_loss:3269.69677734375 \n",
      "train_step 944,\ttest_acc:0.9196000099182129,\ttest_loss:2858.45068359375 \n",
      "train_step 945,\ttest_acc:0.9218999743461609,\ttest_loss:2790.352294921875 \n",
      "train_step 946,\ttest_acc:0.9215999841690063,\ttest_loss:2807.6259765625 \n",
      "train_step 947,\ttest_acc:0.9218000173568726,\ttest_loss:2902.5849609375 \n",
      "train_step 948,\ttest_acc:0.9225000143051147,\ttest_loss:2900.038330078125 \n",
      "train_step 949,\ttest_acc:0.9233999848365784,\ttest_loss:2838.90673828125 \n",
      "train_step 950,\ttest_acc:0.9240999817848206,\ttest_loss:2828.70166015625 \n",
      "train_step 951,\ttest_acc:0.9172000288963318,\ttest_loss:2959.039794921875 \n",
      "train_step 952,\ttest_acc:0.9194999933242798,\ttest_loss:2934.9873046875 \n",
      "train_step 953,\ttest_acc:0.9164999723434448,\ttest_loss:3011.678466796875 \n",
      "train_step 954,\ttest_acc:0.9223999977111816,\ttest_loss:2825.16552734375 \n",
      "train_step 955,\ttest_acc:0.9144999980926514,\ttest_loss:3021.36181640625 \n",
      "train_step 956,\ttest_acc:0.9162999987602234,\ttest_loss:2917.28466796875 \n",
      "train_step 957,\ttest_acc:0.9071999788284302,\ttest_loss:3205.095947265625 \n",
      "train_step 958,\ttest_acc:0.9196000099182129,\ttest_loss:2817.518310546875 \n",
      "train_step 959,\ttest_acc:0.9192000031471252,\ttest_loss:2867.96044921875 \n",
      "train_step 960,\ttest_acc:0.9228000044822693,\ttest_loss:2762.906005859375 \n",
      "train_step 961,\ttest_acc:0.9222000241279602,\ttest_loss:2845.69970703125 \n",
      "train_step 962,\ttest_acc:0.8992000222206116,\ttest_loss:3347.6943359375 \n",
      "train_step 963,\ttest_acc:0.9187999963760376,\ttest_loss:2892.5732421875 \n",
      "train_step 964,\ttest_acc:0.920199990272522,\ttest_loss:2859.348876953125 \n",
      "train_step 965,\ttest_acc:0.906499981880188,\ttest_loss:3212.441650390625 \n",
      "train_step 966,\ttest_acc:0.9147999882698059,\ttest_loss:3099.155029296875 \n",
      "train_step 967,\ttest_acc:0.9186999797821045,\ttest_loss:2925.19775390625 \n",
      "train_step 968,\ttest_acc:0.9150999784469604,\ttest_loss:2942.897216796875 \n",
      "train_step 969,\ttest_acc:0.9200000166893005,\ttest_loss:2820.0419921875 \n",
      "train_step 970,\ttest_acc:0.9157000184059143,\ttest_loss:2982.072265625 \n",
      "train_step 971,\ttest_acc:0.9229000210762024,\ttest_loss:2814.21630859375 \n",
      "train_step 972,\ttest_acc:0.920799970626831,\ttest_loss:2887.2392578125 \n",
      "train_step 973,\ttest_acc:0.9157999753952026,\ttest_loss:3058.011474609375 \n",
      "train_step 974,\ttest_acc:0.9175000190734863,\ttest_loss:2946.75390625 \n",
      "train_step 975,\ttest_acc:0.9225000143051147,\ttest_loss:2825.887451171875 \n",
      "train_step 976,\ttest_acc:0.91839998960495,\ttest_loss:2938.42041015625 \n",
      "train_step 977,\ttest_acc:0.9192000031471252,\ttest_loss:2876.70068359375 \n",
      "train_step 978,\ttest_acc:0.9218000173568726,\ttest_loss:2864.08349609375 \n",
      "train_step 979,\ttest_acc:0.9196000099182129,\ttest_loss:2913.69873046875 \n",
      "train_step 980,\ttest_acc:0.921500027179718,\ttest_loss:2811.32666015625 \n",
      "train_step 981,\ttest_acc:0.9180999994277954,\ttest_loss:2890.47119140625 \n",
      "train_step 982,\ttest_acc:0.9174000024795532,\ttest_loss:2898.767578125 \n",
      "train_step 983,\ttest_acc:0.9211000204086304,\ttest_loss:2805.15869140625 \n",
      "train_step 984,\ttest_acc:0.9114999771118164,\ttest_loss:3058.767822265625 \n",
      "train_step 985,\ttest_acc:0.9169999957084656,\ttest_loss:2905.658203125 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_step 986,\ttest_acc:0.9199000000953674,\ttest_loss:2860.3212890625 \n",
      "train_step 987,\ttest_acc:0.9243000149726868,\ttest_loss:2799.251953125 \n",
      "train_step 988,\ttest_acc:0.9176999926567078,\ttest_loss:2915.316162109375 \n",
      "train_step 989,\ttest_acc:0.8973000049591064,\ttest_loss:3581.250732421875 \n",
      "train_step 990,\ttest_acc:0.9211000204086304,\ttest_loss:2853.40625 \n",
      "train_step 991,\ttest_acc:0.9172999858856201,\ttest_loss:2971.495361328125 \n",
      "train_step 992,\ttest_acc:0.9168999791145325,\ttest_loss:2922.93603515625 \n",
      "train_step 993,\ttest_acc:0.9150999784469604,\ttest_loss:2978.592529296875 \n",
      "train_step 994,\ttest_acc:0.9049999713897705,\ttest_loss:3269.58544921875 \n",
      "train_step 995,\ttest_acc:0.9228000044822693,\ttest_loss:2838.934814453125 \n",
      "train_step 996,\ttest_acc:0.9187999963760376,\ttest_loss:2912.151611328125 \n",
      "train_step 997,\ttest_acc:0.9194999933242798,\ttest_loss:2847.461669921875 \n",
      "train_step 998,\ttest_acc:0.9200000166893005,\ttest_loss:2880.917724609375 \n",
      "train_step 999,\ttest_acc:0.9085999727249146,\ttest_loss:3144.689208984375 \n",
      "train_step 1000,\ttest_acc:0.9229000210762024,\ttest_loss:2818.310791015625 \n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "for i in range(1000):\n",
    "    # 每次取100个数据\n",
    "    batch_x, batch_y = mnist.train.next_batch(100)\n",
    "    # 在Session中运行train_step,运行时要传入占位符的值\n",
    "    sess.run(train_step,feed_dict={x:batch_x,y_:batch_y})\n",
    "    # eval(feed_dict=None,session=None) Evaluates this tensor in a Session.\n",
    "    # Calling this method will execute all preceding operations that produce the inputs needed for the operation that produces this tensor.\n",
    "    test_loss = cross_entropy.eval(feed_dict={x:mnist.test.images,y_:mnist.test.labels},session=sess)\n",
    "    test_acc = accuracy.eval(feed_dict={x:mnist.test.images,y_:mnist.test.labels})\n",
    "    print(\"train_step {},\\ttest_acc:{},\\ttest_loss:{} \".format(i+1,test_acc,test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.917\n",
      "2983.8213\n"
     ]
    }
   ],
   "source": [
    "# 计算模型在测试集上的准确率\n",
    "print(sess.run(accuracy,feed_dict={x:mnist.test.images,y_:mnist.test.labels}))\n",
    "# 计算模型在测试集上的交叉熵\n",
    "print(sess.run(cross_entropy,feed_dict={x:mnist.test.images,y_:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
